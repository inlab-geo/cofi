{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linear regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In\nColab](https://img.shields.io/badge/open%20in-Colab-b5e2fa?logo=googlecolab&style=flat-square&color=ffd670)](https://colab.research.google.com/github/inlab-geo/cofi-examples/blob/main/tutorials/linear_regression/linear_regression.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n\n# What we do in this notebook\n\nHere we demonstrate use of CoFI on a simple **linear regression**\nproblem, where we fit a polynomial function to data, using three\ndifferent algorithms:\n\n-   by solution of a linear system of equations,\n-   by optimization of a data misfit function\n-   by Bayesian sampling of a Likelihood multiplied by a prior.\n\n------------------------------------------------------------------------\n\n# Learning outcomes\n\n-   A demonstration of running CoFI for a class of parameter fitting\n    problem. Example of a CoFI **template**.\n-   A demonstration of how CoFI may be used to **experiment with\n    different inference approaches** under a common interface.\n-   A demonstration of CoFI's **expandability** in that it may be used\n    with pre-set, or user defined, misfits, likelihood or priors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Environment setup (uncomment code below)\n\n# !pip install -U cofi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linear regression\n\nLets start with some (x,y) data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# here is some (x,y) data\ndata_x = np.array([1.1530612244897958, -0.07142857142857162, -1.7857142857142858, \n                1.6428571428571423, -2.642857142857143, -1.0510204081632653, \n                1.1530612244897958, -1.295918367346939, -0.806122448979592, \n                -2.2755102040816326, -2.2755102040816326, -0.6836734693877551, \n                0.7857142857142856, 1.2755102040816322, -0.6836734693877551, \n                -3.2551020408163267, -0.9285714285714288, -3.377551020408163, \n                -0.6836734693877551, 1.7653061224489797])\n\ndata_y = np.array([-7.550931153863841, -6.060810406314714, 3.080063056254076, \n                -4.499764131508964, 2.9462042659962333, -0.4645899453212615, \n                -7.43068837808917, 1.6273774547833582, -0.05922697815443567, \n                3.8462283231266903, 3.425851020301113, -0.05359797104829345, \n                -10.235538857712598, -5.929113775071286, -1.1871766078924957, \n                -4.124258811692425, 0.6969191559961637, -4.454022624935177, \n                -2.352842192972056, -4.25145590011172])\nsigma = 1   # estimation on the data noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now lets plot the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_data(sigma=None):\n    if(sigma is None):\n        plt.scatter(data_x, data_y, color=\"lightcoral\", label=\"observed data\")\n    else:\n        plt.errorbar(data_x, data_y, yerr=sigma, fmt='.',color=\"lightcoral\",ecolor='lightgrey',ms=10)\nplot_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Problem description\n\nTo begin with, we will work with polynomial curves,\n\n$$y(x) = \\sum_{j=0}^M m_j x^j\\,.$$\n\nHere, $M$ is the 'order' of the polynomial: if $M=1$ we have a straight\nline with 2 parameters, if $M=2$ it will be a quadratic with 3\nparameters, and so on. The $m_j, (j=0,\\dots M)$ are the 'model\ncoefficients' that we seek to constrain from the data.\n\nFor this class of problem the forward operator takes the following form:\n\n$$\\begin{aligned}\n\\left(\\begin{array}{c}y_0\\\\y_1\\\\\\vdots\\\\y_N\\end{array}\\right) = \\left(\\begin{array}{ccc}1&x_0&x_0^2&x_0^3\\\\1&x_1&x_1^2&x_1^3\\\\\\vdots&\\vdots&\\vdots\\\\1&x_N&x_N^2&x_N^3\\end{array}\\right)\\left(\\begin{array}{c}m_0\\\\m_1\\\\m_2\\\\m_3\\end{array}\\right)\n\\end{aligned}$$\n\nThis clearly has the required general form, $\\mathbf{d} =G{\\mathbf m}$.\n\nwhere:\n\n-   $\\textbf{d}$ is the vector of data values, ($y_0,y_1,\\dots,y_N$);\n-   $\\textbf{m}$ is the vector of model parameters, ($m_0,m_1,m_2$);\n-   $G$ is the basis matrix (or design matrix) of this linear regression\n    problem (also called the **Jacobian** matrix for this linear\n    problem).\n\nWe have a set of noisy data values, $y_i (i=0,\\dots,N)$, measured at\nknown locations, $x_i (i=0,\\dots,N)$, and wish to find the best fit\ndegree 3 polynomial.\n\nThe function that generated our data is : $y=-6-5x+2x^2+x^3$, and we\nhave added Gaussian random noise, ${\\cal N}(0,\\sigma^2)$, with\n$\\sigma=1.0$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now build the Jacobian/G matrix for this problem and define a forward\nfunction which simply multiplies $\\mathbf m$ by $G$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nparams = 4 # Number of model parameters to be solved for\n\ndef jacobian(x=data_x, n=nparams):\n    return np.array([x**i for i in range(n)]).T\n\ndef forward(model):\n    return jacobian().dot(model)\n\ndef Cd_inv(sigma=sigma, ndata=len(data_x)):\n    return 1/sigma**2 * np.identity(ndata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the true model for later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# True model for plotting\nx = np.linspace(-3.5,2.5)              # x values to plot\ntrue_model = np.array([-6, -5, 2, 1])  # we know it for this case which will be useful later for comparison.\n\ntrue_y = jacobian(x,4).dot(true_model) # y values for true curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets plot the data with the curve from the true polynomial\ncoefficients.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Some plotting utilities\ndef plot_model(x,y, label, color=None):\n    #x = np.linspace(-3.5,2.5)\n    #y = jacobian(x).dot(model)\n    plt.plot(x, y, color=color or \"green\", label=label)\n    plt.xlabel(\"X\")\n    plt.ylabel(\"Y\")\n    plt.legend()\n\ndef plot_models(models, label=\"Posterior samples\", color=\"seagreen\", alpha=0.1):\n    x = np.linspace(-3.5,2.5)\n    G = jacobian(x)\n    plt.plot(x, G.dot(models[0]), color=color, label=label, alpha=alpha)\n    for m in models:\n        plt.plot(x, G.dot(m), color=color, alpha=alpha)\n    plt.xlabel(\"X\")\n    plt.ylabel(\"Y\")\n    plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_data(sigma=sigma)\nplot_model(x,true_y, \"true model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we have the data and the forward model we can start to try and\nestimate the coefficients of the polynomial from the data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The structure of CoFI\n\nIn the workflow of `cofi`, there are three main components:\n`BaseProblem`, `InversionOptions`, and `Inversion`.\n\n-   `BaseProblem` defines the inverse problem including any user\n    supplied quantities such as data vector, number of model parameters\n    and measure of fit between model predictions and data.\n\n    ``` python\n    inv_problem = BaseProblem()\n    inv_problem.set_objective(some_function_here)\n    inv_problem.set_jacobian(some_function_here)\n    inv_problem.set_initial_model(a_starting_point) # if needed, e.g. we are solving a nonlinear problem by optimization\n    ```\n\n    \u00a0\n\n-   `InversionOptions` describes details about how one wants to run the\n    inversion, including the backend tool and solver-specific\n    parameters. It is based on the concept of a `method` and `tool`.\n\n    ``` python\n    inv_options = InversionOptions()\n    inv_options.suggest_solving_methods()\n    inv_options.set_solving_method(\"matrix solvers\")\n    inv_options.suggest_tools()\n    inv_options.set_tool(\"scipy.linalg.lstsq\")\n    inv_options.summary()\n    ```\n\n    \u00a0\n\n-   `Inversion` can be seen as an inversion engine that takes in the\n    above two as information, and will produce an `InversionResult` upon\n    running.\n\n    ``` python\n    inv = Inversion(inv_problem, inv_options)\n    result = inv.run()\n    ```\n\nInternally CoFI decides the nature of the problem from the quantities\nset by the user and performs internal checks to ensure it has all that\nit needs to solve a problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Linear system solver\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cofi import BaseProblem, InversionOptions, Inversion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 1. Define CoFI `BaseProblem`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inv_problem = BaseProblem()\ninv_problem.set_data(data_y)\ninv_problem.set_jacobian(jacobian())\ninv_problem.set_data_covariance_inv(Cd_inv())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 2. Define CoFI `InversionOptions`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inv_options = InversionOptions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the information supplied, we can ask CoFI to suggest some solving\nmethods.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inv_options.suggest_solving_methods()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can ask CoFI to suggest some specific software tools as well.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inv_options.suggest_tools()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inv_options.set_solving_method(\"matrix solvers\") # lets decide to use a matrix solver.\ninv_options.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# below is optional, as this has already been the default tool under \"linear least square\"\ninv_options.set_tool(\"scipy.linalg.lstsq\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 3. Define CoFI `Inversion` and run\n\nOur choices so far have defined a linear parameter estimation problem\n(without any regularization) to be solved within a least squares\nframework. In this case the selection of a `matrix solvers` method will\nmean we are calculating the standard least squares solution\n\n$$m = (G^T C_d^{-1} G)^{-1} G^T C_d^{-1} d$$\n\nand our choice of backend tool `scipy.linalg.lstsq`, means that we will\nemploy scipy's `linalg` package to perform the numerics.\n\nLets run CoFI.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inv = Inversion(inv_problem, inv_options)\ninv_result = inv.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"The inversion result from `scipy.linalg.lstsq`: {inv_result.model}\\n\")\ninv_result.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets plot the solution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_data()\nplot_model(x,jacobian(x).dot(inv_result.model), \"linear system solver\", color=\"seagreen\")\nplot_model(x,true_y, \"true model\", color=\"darkorange\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Optimizer\n\nThe same overdetermined linear problem, $\\textbf{d} = G\\textbf{m}$, with\nGaussian data noise can also be solved by minimising the squares of the\nresidual of the linear equations,\ne.g.\u00a0$\\textbf{r}^T \\textbf{C}_d^{-1}\\textbf{r}$ where\n$\\textbf{r}=\\textbf{d}-G\\textbf{m}$. The above matrix solver solution\ngives us the best data fitting model, but a direct optimisation approach\ncould also be used, say when the number of unknowns is large and we do\nnot wish, or are unable to provide the Jacobian function.\n\nSo we use a plain optimizer `scipy.optimize.minimize` to demonstrate\nthis ability.\n\n```{=html}\n<!-- For this backend solver to run successfully, some additional information should be provided, otherwise\nyou'll see an error to notify what additional information is required by the solver.\n\nThere are several ways to provide the information needed to solve an inverse problem with \nCoFI. In the example below we provide functions to calculate the data and the optional \nregularisation. CoFI then generates the objective function for us based on the information \nprovided. The alternative to this would be to directly provide objective function to CoFI. -->\n```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "######## CoFI BaseProblem - provide additional information\ninv_problem.set_initial_model(np.ones(nparams))\ninv_problem.set_forward(forward)\ninv_problem.set_data_misfit(\"squared error\")\n\n# inv_problem.set_objective(your_own_misfit_function)    # (optionally) if you'd like to define your own misfit\n# inv_problem.set_gradient(your_own_gradient_of_misfit_function)    # (optionally) if you'd like to define your own misfit gradient\n\n######## CoFI InversionOptions - set a different tool\ninv_options_2 = InversionOptions()\ninv_options_2.set_tool(\"scipy.optimize.minimize\")\ninv_options_2.set_params(method=\"Nelder-Mead\")\n\n######## CoFI Inversion - run it\ninv_2 = Inversion(inv_problem, inv_options_2)\ninv_result_2 = inv_2.run()\n\n######## CoFI InversionResult - check result\nprint(f\"The inversion result from `scipy.optimize.minimize`: {inv_result_2.model}\\n\")\ninv_result_2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_data()\nplot_model(x,jacobian(x).dot(inv_result_2.model), \"optimization solution\", color=\"cornflowerblue\")\nplot_model(x,true_y, \"true model\", color=\"darkorange\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Challenge: Change the polynomial degree\n\nTry and replace the 3rd order polynomial with a 1st order polynomial\n(i.e.\u00a0$M=1$) by adding the required commands below. What does the plot\nlooks like?\n\n[![Upload to Jamboard\n1](https://img.shields.io/badge/Click%20&%20upload%20your%20results%20to-Jamboard-lightgrey?logo=jamboard&style=for-the-badge&color=fcbf49&labelColor=edede9)](https://jamboard.google.com/d/1Fu_vIhWIlDl-gs9gzSPBNXLjzj2CsS70fLMDN8-7Sew/edit?usp=sharing)\n\nStart from code below:\n\n    inv_problem = BaseProblem()\n    inv_problem.set_data(data_y)\n    inv_problem.set_jacobian(jacobian(n=<CHANGE ME>))\n    inv_problem.set_data_covariance_inv(Cd_inv())\n    inv_options.set_solving_method(\"matrix solvers\") # lets decide to use a matrix solver.\n    inv = Inversion(inv_problem, inv_options)\n    inv_result = inv.run()\n\n    print(\"Inferred curve with n = <CHANGE ME> \")\n    plot_data()\n    plot_model(x,jacobian(x,n=<CHANGE ME>).dot(inv_result.model), \"optimization solution\", color=\"cornflowerblue\")\n    plot_model(x,true_y, \"true model\", color=\"darkorange\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Copy the template above, Replace <CHANGE ME> with your answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#@title Solution\n\ninv_problem = BaseProblem()\ninv_problem.set_data(data_y)\ninv_problem.set_jacobian(jacobian(n=2))\ninv_problem.set_data_covariance_inv(Cd_inv())\ninv_options.set_solving_method(\"matrix solvers\") # lets decide to use a matrix solver.\ninv = Inversion(inv_problem, inv_options)\ninv_result = inv.run()\n\nprint(\"Inferred curve with n = 2 \")\nplot_data()\nplot_model(x,jacobian(x,n=2).dot(inv_result.model), \"optimization solution\", color=\"cornflowerblue\")\nplot_model(x,true_y, \"true model\", color=\"darkorange\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Bayesian sampling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Likelihood\n\nSince data errors follow a Gaussian in this example, we can define a\nLikelihood function, $p({\\mathbf d}_{obs}| {\\mathbf m})$.\n\n$$p({\\mathbf d}_{obs} | {\\mathbf m}) \\propto \\exp \\left\\{- \\frac{1}{2} ({\\mathbf d}_{obs}-{\\mathbf d}_{pred}({\\mathbf m}))^T C_D^{-1} ({\\mathbf d}_{obs}-{\\mathbf d}_{pred}({\\mathbf m})) \\right\\}$$\n\nwhere ${\\mathbf d}_{obs}$ represents the observed y values and\n${\\mathbf d}_{pred}({\\mathbf m})$ are those predicted by the polynomial\nmodel $({\\mathbf m})$. The Likelihood is defined as the probability of\nobserving the data actually observed, given a model. In practice we\nusually only need to evaluate the log of the Likelihood,\n$\\log p({\\mathbf d}_{obs} | {\\mathbf m})$. To do so, we require the\ninverse data covariance matrix describing the statistics of the noise in\nthe data, $C_D^{-1}$ . For this problem the data errors are independent\nwith identical standard deviation in noise for each datum. Hence\n$C_D^{-1} = \\frac{1}{\\sigma^2}I$ where $\\sigma=1$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sigma = 1.0                                     # common noise standard deviation\nCdinv = np.eye(len(data_y))/(sigma**2)      # inverse data covariance matrix\n\ndef log_likelihood(model):\n    y_synthetics = forward(model)\n    residual = data_y - y_synthetics\n    return -0.5 * residual @ (Cdinv @ residual).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the user could specify **any appropriate Likelihood function**\nof their choosing here.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prior\n\nBayesian sampling requires a prior probability density function. A\ncommon problem with polynomial coefficients as model parameters is that\nit is not at all obvious what a prior should be. Here we choose a\nuniform prior with specified bounds\n\n$$\\begin{aligned}\n\\begin{align}\np({\\mathbf m}) &= \\frac{1}{V},\\quad  l_i \\le m_i \\le u_i, \\quad (i=1,\\dots,M)\\\\\n\\\\\n         &= 0, \\quad {\\rm otherwise},\n\\end{align}\n\\end{aligned}$$\n\nwhere $l_i$ and $u_i$ are lower and upper bounds on the $i$th model\ncoefficient.\n\nHere use the uniform distribution with\n${\\mathbf l}^T = (-10.,-10.,-10.,-10.)$, and\n${\\mathbf u}^T = (10.,10.,10.,10.)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "m_lower_bound = np.ones(nparams) * (-10.)             # lower bound for uniform prior\nm_upper_bound = np.ones(nparams) * 10                 # upper bound for uniform prior\n\ndef log_prior(model):    # uniform distribution\n    for i in range(len(m_lower_bound)):\n        if model[i] < m_lower_bound[i] or model[i] > m_upper_bound[i]: return -np.inf\n    return 0.0 # model lies within bounds -> return log(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the user could specify **any appropriate Prior PDF** of their\nchoosing here.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bayesian sampling\n\nIn this aproach we sample a probability distribution rather than find a\nsingle best fit solution. Bayes' theorem tells us the the posterior\ndistribution is proportional to the Likelihood and the prior.\n\n$$p(\\mathbf{m}|\\mathbf{d}) = K p(\\mathbf{d}|\\mathbf{m})p(\\mathbf{m})$$\n\nwhere $K$ is some constant. Under the assumptions specified\n$p(\\mathbf{m}|\\mathbf{d})$ gives a probability density of models that\nare supported by the data. We seek to draw random samples from\n$p(\\mathbf{m}|\\mathbf{d})$ over model space and then to make inferences\nfrom the resulting ensemble of model parameters.\n\nIn this example we make use of *The Affine Invariant Markov chain Monte\nCarlo (MCMC) Ensemble sampler* [Goodman and Weare\n2010](https://msp.org/camcos/2010/5-1/p04.xhtml) to sample the posterior\ndistribution of the model. (See more details about\n[emcee](https://emcee.readthedocs.io/en/stable/)).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Starting points for random walkers\n\nNow we define some hyperparameters (e.g.\u00a0the number of walkers and\nsteps), and initialise the starting positions of walkers. We start all\nwalkers in a small ball about a chosen point $(0, 0, 0, 0)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nwalkers = 32\nndim = nparams\nnsteps = 10000\nwalkers_start = np.zeros(nparams) + 1e-4 * np.random.randn(nwalkers, ndim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Add the information and run with CoFI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "######## CoFI BaseProblem - provide additional information\ninv_problem.set_log_prior(log_prior)\ninv_problem.set_log_likelihood(log_likelihood)\ninv_problem.set_model_shape(ndim)\n\n######## CoFI InversionOptions - get a different tool\ninv_options_3 = InversionOptions()\ninv_options_3.set_tool(\"emcee\")      # Here we use to Affine Invariant McMC sampler from Goodman and Weare (2010).\ninv_options_3.set_params(nwalkers=nwalkers, nsteps=nsteps, initial_state=walkers_start, progress=True)\n\n######## CoFI Inversion - run it\ninv_3 = Inversion(inv_problem, inv_options_3)\ninv_result_3 = inv_3.run()\n\n######## CoFI InversionResult - check result\nprint(f\"The inversion result from `emcee`:\")\ninv_result_3.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Post-sampling analysis\n\nBy default the raw sampler resulting object is attached to `cofi`'s\ninversion result.\n\nOptionally, you can convert that into an `arviz` data structure to have\naccess to a range of analysis functions. (See more details in [arviz\ndocumentation](https://python.arviz.org/en/latest/index.html)).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import arviz as az\n\nlabels = [\"m0\", \"m1\", \"m2\",\"m3\"]\n\nsampler = inv_result_3.sampler\naz_idata = az.from_emcee(sampler, var_names=labels)\n# az_idata = inv_result_3.to_arviz()      # alternatively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "az_idata.get(\"posterior\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# a standard `trace` plot\naxes = az.plot_trace(az_idata, backend_kwargs={\"constrained_layout\":True}); \n\n# add legends\nfor i, axes_pair in enumerate(axes):\n    ax1 = axes_pair[0]\n    ax2 = axes_pair[1]\n    ax1.axvline(true_model[i], linestyle='dotted', color='red')\n    ax1.set_xlabel(\"parameter value\")\n    ax1.set_ylabel(\"density value\")\n    ax2.set_xlabel(\"number of iterations\")\n    ax2.set_ylabel(\"parameter value\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tau = sampler.get_autocorr_time()\nprint(f\"autocorrelation time: {tau}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# a Corner plot\n\nfig, axes = plt.subplots(nparams, nparams, figsize=(12,8))\n\nif(False): # if we are plotting the model ensemble use this\n    az.plot_pair(\n        az_idata.sel(draw=slice(300,None)), \n        marginals=True, \n        reference_values=dict(zip([f\"m{i}\" for i in range(4)], true_model.tolist())),\n        ax=axes,\n    );\nelse: # if we wish to plot a kernel density plot then use this option\n    az.plot_pair(\n        az_idata.sel(draw=slice(300,None)), \n        marginals=True, \n        reference_values=dict(zip([f\"m{i}\" for i in range(4)], true_model.tolist())),\n        kind=\"kde\",\n        kde_kwargs={\n            \"hdi_probs\": [0.3, 0.6, 0.9],  # Plot 30%, 60% and 90% HDI contours\n            \"contourf_kwargs\": {\"cmap\": \"Blues\"},\n        },\n        ax=axes,\n    );"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we plot the predicted curves for the posterior ensemble of\nsolutions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "flat_samples = sampler.get_chain(discard=300, thin=30, flat=True)\ninds = np.random.randint(len(flat_samples), size=100) # get a random selection from posterior ensemble\n\nplot_data()\nplot_models(flat_samples[inds])\nplot_model(x,true_y, \"True model\", color=\"darkorange\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Expected values, credible intervals and model covariance matrix from the ensemble\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n Expected value and 95% credible intervals \")\nfor i in range(ndim):\n    mcmc = np.percentile(flat_samples[:, i], [5, 50, 95])\n    print(\" {} {:7.3f} [{:7.3f}, {:7.3f}]\".format(labels[i],mcmc[1],mcmc[0],mcmc[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "CMpost = np.cov(flat_samples.T)\nCM_std= np.std(flat_samples,axis=0)\nprint('Posterior model covariance matrix\\n',CMpost)\nprint('\\n Posterior estimate of model standard deviations in each parameter')\nfor i in range(ndim):\n    print(\"    {} {:7.4f}\".format(labels[i],CM_std[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Challenge: Change the prior model bounds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Replace the previous prior bounds to new values\n\nThe original uniform bounds had\n\n${\\mathbf l}^T = (-10.,-10.,-10.,-10.)$, and\n${\\mathbf u}^T = (10.,10.,10.,10.)$.\n\nLets replace with\n\n${\\mathbf l}^T = (-1.,-10.,-10.,-10.)$, and\n${\\mathbf u}^T = (2.,10.,10.,10.)$.\n\nWe have only changed the bounds of the first parameter. However since\nthe true value of constant term was 6, these bounds are now inconsistent\nwith the true model.\n\nWhat does this do to the posterior distribution?\n\n[![Upload to Jamboard\n2](https://img.shields.io/badge/Click%20&%20upload%20your%20results%20to-Jamboard-lightgrey?logo=jamboard&style=for-the-badge&color=fcbf49&labelColor=edede9)](https://jamboard.google.com/d/1h_O8PNuHzpyH2zQUraqiMT4SQR0TMhUmiZzFn_HMZl4/edit?usp=sharing)\n\nStart from the code template below:\n\n    m_lower_bound = <CHANGE ME>             # lower bound for uniform prior\n    m_upper_bound = <CHANGE ME>             # upper bound for uniform prior\n\n    def log_prior(model):    # uniform distribution\n        for i in range(len(m_lower_bound)):\n            if model[i] < m_lower_bound[i] or model[i] > m_upper_bound[i]: return -np.inf\n        return 0.0 # model lies within bounds -> return log(1)\n\n    ######## CoFI BaseProblem - update information\n    inv_problem.set_log_prior(log_prior)\n\n    ######## CoFI Inversion - run it\n    inv_4 = Inversion(inv_problem, inv_options_3)\n    inv_result_4 = inv_4.run()\n\n    flat_samples = inv_result_4.sampler.get_chain(discard=300, thin=30, flat=True)\n    inds = np.random.randint(len(flat_samples), size=100) # get a random selection from posterior ensemble\n\n    print(\"Resulting samples with prior model lower bounds of <CHANGE ME>, upper bounds of <CHANGE ME>\")\n    plot_data()\n    plot_models(flat_samples[inds])\n    plot_model(x, true_y, \"True model\", color=\"darkorange\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Copy the template above, Replace <CHANGE ME> with your answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#@title Solution\n\nm_lower_bound = np.array([-1,-10,-10,-10])             # lower bound for uniform prior\nm_upper_bound = np.array([2,10,10,10])                 # upper bound for uniform prior\n\ndef log_prior(model):    # uniform distribution\n    for i in range(len(m_lower_bound)):\n        if model[i] < m_lower_bound[i] or model[i] > m_upper_bound[i]: return -np.inf\n    return 0.0 # model lies within bounds -> return log(1)\n\n######## CoFI BaseProblem - update information\ninv_problem.set_log_prior(log_prior)\n\n######## CoFI Inversion - run it\ninv_4 = Inversion(inv_problem, inv_options_3)\ninv_result_4 = inv_4.run()\n\nflat_samples = inv_result_4.sampler.get_chain(discard=300, thin=30, flat=True)\ninds = np.random.randint(len(flat_samples), size=100) # get a random selection from posterior ensemble\n\nprint(\"Resulting samples with prior model lower bounds of [-1,-10,-10,-10], upper bounds of [2,10,10,10]\")\nplot_data()\nplot_models(flat_samples[inds])\nplot_model(x, true_y, \"True model\", color=\"darkorange\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Why do you think the posterior distribution looks like this?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Challenge: Change the data uncertainty\n\nTo change the data uncertainty we increase `sigma` and then redefine the\nlog-Likelihood.\n\nHere we increase the assumed data standard deviation by a factor of of\n50! So we are telling the inversion that the data are far less accurate\nthan they actually are.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sigma = 50.0                                     # common noise standard deviation\nCdinv = np.eye(len(data_y))/(sigma**2)      # inverse data covariance matrix\n\ndef log_likelihood(model):\n    y_synthetics = forward(model)\n    residual = data_y - y_synthetics\n    return -0.5 * residual @ (Cdinv @ residual).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets return the prior to the original bounds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "m_lower_bound = np.ones(4) * (-10.)             # lower bound for uniform prior\nm_upper_bound = np.ones(4) * 10                 # upper bound for uniform prior\n\ndef log_prior(model):    # uniform distribution\n    for i in range(len(m_lower_bound)):\n        if model[i] < m_lower_bound[i] or model[i] > m_upper_bound[i]: return -np.inf\n    return 0.0 # model lies within bounds -> return log(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Your challenge is then to tell CoFI that the Likelihood and prior have\nchanged and then to rerun the sample, and plot results.\n\n[![Upload to Jamboard\n3](https://img.shields.io/badge/Click%20&%20upload%20your%20results%20to-Jamboard-lightgrey?logo=jamboard&style=for-the-badge&color=fcbf49&labelColor=edede9)](https://jamboard.google.com/d/1ewIkma6uTeNWu7ACEC3vG4J0FNPQZVLdlQLhyeLh-qM/edit?usp=sharing)\n\nFeel free to start from the code below:\n\n    ######## CoFI BaseProblem - update information\n    inv_problem.set_log_likelihood(<CHANGE ME>)\n    inv_problem.set_log_prior(<CHANGE ME>)\n\n    ######## CoFI Inversion - run it\n    inv_5 = Inversion(inv_problem, inv_options_3)\n    inv_result_5 = inv_5.run()\n\n    flat_samples = inv_result_5.sampler.get_chain(discard=300, thin=30, flat=True)\n    inds = np.random.randint(len(flat_samples), size=100) # get a random selection from posterior ensemble\n\n    print(\"Resulting samples from changed data uncertainty\")\n    plot_data()\n    plot_models(flat_samples[inds])\n    plot_model(x,true_y, \"True model\", color=\"darkorange\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Copy the template above, Replace <CHANGE ME> with your answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The answer is in the next cells if you want to run them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#@title Solution\n\n######## CoFI BaseProblem - update information\ninv_problem.set_log_likelihood(log_likelihood)\ninv_problem.set_log_prior(log_prior)\n\n######## CoFI Inversion - run it\ninv_5 = Inversion(inv_problem, inv_options_3)\ninv_result_5 = inv_5.run()\n\nflat_samples = inv_result_5.sampler.get_chain(discard=300, thin=30, flat=True)\ninds = np.random.randint(len(flat_samples), size=100) # get a random selection from posterior ensemble\n\nprint(\"Resulting samples from changed data uncertainty\")\nplot_data()\nplot_models(flat_samples[inds])\nplot_model(x,true_y, \"True model\", color=\"darkorange\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Challenge: Change the number of walkers / steps in the McMC algorithm (optional)\n\nNow lets decrease the number of steps performed by the McMC algorithm.\nIt will be faster but perform less exploration of the model parameters.\n\nWe suggest you reduce the number of steps taken by all 32 random walkers\nand see how it affects the posterior ensemble.\n\n[![Upload to Jamboard\n4](https://img.shields.io/badge/Click%20&%20upload%20your%20results%20to-Jamboard-lightgrey?logo=jamboard&style=for-the-badge&color=fcbf49&labelColor=edede9)](https://jamboard.google.com/d/1vAm3dpaI4UTZiFXzb6vEku8AlVWUw7PRxz8KJk-dVf8/edit?usp=sharing)\n\nYou can start from code template below:\n\n    # change number of steps\n    nsteps = <CHANGE ME>              # instead of 10000\n\n    # change number of walkers\n    nwalkers = <CHANGE ME>            # instead of 32\n    walkers_start = np.zeros(nparams) + 1e-4 * np.random.randn(nwalkers, ndim)\n\n    # let's return to the old uncertainty settings\n    sigma = 1.0                                     # common noise standard deviation\n    Cdinv = np.eye(len(data_y))/(sigma**2)      # inverse data covariance matrix\n\n    ######## CoFI InversionOptions - get a different tool\n    inv_options_3.set_params(nsteps=nsteps, nwalkers=nwalkers, initial_state=walkers_start)\n\n    ######## CoFI Inversion - run it\n    inv_6 = Inversion(inv_problem, inv_options_3)\n    inv_result_6 = inv_6.run()\n\n    ######## CoFI InversionResult - plot result\n    flat_samples = inv_result_6.sampler.get_chain(discard=300, thin=30, flat=True)\n    inds = np.random.randint(len(flat_samples), size=10) # get a random selection from posterior ensemble\n\n    print(f\"Inference results from {nsteps} steps and {nwalkers} walkers\")\n    plot_data()\n    plot_models(flat_samples[inds])\n    plot_model(x,true_y, \"True model\", color=\"darkorange\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Copy the template above, Replace <CHANGE ME> with your answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#@title Solution\n\n# change number of steps\nnsteps = 400              # instead of 10000\n\n# change number of walkers\nnwalkers = 30             # instead of 32\nwalkers_start = np.zeros(nparams) + 1e-4 * np.random.randn(nwalkers, ndim)\n\n# let's return to the old uncertainty settings\nsigma = 1.0                                     # common noise standard deviation\nCdinv = np.eye(len(data_y))/(sigma**2)      # inverse data covariance matrix\n\n######## CoFI InversionOptions - get a different tool\ninv_options_3.set_params(nsteps=nsteps, nwalkers=nwalkers, initial_state=walkers_start)\n\n######## CoFI Inversion - run it\ninv_6 = Inversion(inv_problem, inv_options_3)\ninv_result_6 = inv_6.run()\n\n######## CoFI InversionResult - plot result\nflat_samples = inv_result_6.sampler.get_chain(discard=300, thin=30, flat=True)\ninds = np.random.randint(len(flat_samples), size=10) # get a random selection from posterior ensemble\n\nprint(f\"Inference results from {nsteps} steps and {nwalkers} walkers\")\nplot_data()\nplot_models(flat_samples[inds])\nplot_model(x,true_y, \"True model\", color=\"darkorange\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n\n# Where to next?\n\n-   Linear regression with Eustatic Sea-level data - [link to\n    notebook](https://github.com/inlab-geo/cofi-examples/blob/main/examples/linear_regression/linear_regression_sealevel.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n\n# Watermark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "watermark_list = [\"cofi\", \"numpy\", \"scipy\", \"matplotlib\", \"emcee\", \"arviz\"]\nfor pkg in watermark_list:\n    pkg_var = __import__(pkg)\n    print(pkg, getattr(pkg_var, \"__version__\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "sphinx_gallery_thumbnail_number = -1\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}