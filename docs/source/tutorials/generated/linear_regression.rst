
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "tutorials/generated/linear_regression.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_tutorials_generated_linear_regression.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorials_generated_linear_regression.py:


Linear regression
=================

.. GENERATED FROM PYTHON SOURCE LINES 9-14

|Open In Colab|

.. |Open In Colab| image:: https://img.shields.io/badge/open%20in-Colab-b5e2fa?logo=googlecolab&style=flat-square&color=ffd670
   :target: https://colab.research.google.com/github/inlab-geo/cofi-examples/blob/main/tutorials/linear_regression/linear_regression.ipynb


.. GENERATED FROM PYTHON SOURCE LINES 17-42

--------------

What we do in this notebook
---------------------------

Here we demonstrate use of CoFI on a simple **linear regression**
problem, where we fit a polynomial function to data, using three
different algorithms:

-  by solution of a linear system of equations,
-  by optimization of a data misfit function
-  by Bayesian sampling of a Likelihood multiplied by a prior.

--------------

Learning outcomes
-----------------

-  A demonstration of running CoFI for a class of parameter fitting
   problem. Example of a CoFI **template**.
-  A demonstration of how CoFI may be used to **experiment with
   different inference approaches** under a common interface.
-  A demonstration of CoFI’s **expandability** in that it may be used
   with pre-set, or user defined, misfits, likelihood or priors.


.. GENERATED FROM PYTHON SOURCE LINES 42-47

.. code-block:: Python


    # Environment setup (uncomment code below)

    # !pip install -U cofi








.. GENERATED FROM PYTHON SOURCE LINES 52-57

Linear regression
-----------------

Lets start with some (x,y) data.


.. GENERATED FROM PYTHON SOURCE LINES 57-61

.. code-block:: Python


    import numpy as np
    import matplotlib.pyplot as plt








.. GENERATED FROM PYTHON SOURCE LINES 63-82

.. code-block:: Python


    # here is some (x,y) data
    data_x = np.array([1.1530612244897958, -0.07142857142857162, -1.7857142857142858, 
                    1.6428571428571423, -2.642857142857143, -1.0510204081632653, 
                    1.1530612244897958, -1.295918367346939, -0.806122448979592, 
                    -2.2755102040816326, -2.2755102040816326, -0.6836734693877551, 
                    0.7857142857142856, 1.2755102040816322, -0.6836734693877551, 
                    -3.2551020408163267, -0.9285714285714288, -3.377551020408163, 
                    -0.6836734693877551, 1.7653061224489797])

    data_y = np.array([-7.550931153863841, -6.060810406314714, 3.080063056254076, 
                    -4.499764131508964, 2.9462042659962333, -0.4645899453212615, 
                    -7.43068837808917, 1.6273774547833582, -0.05922697815443567, 
                    3.8462283231266903, 3.425851020301113, -0.05359797104829345, 
                    -10.235538857712598, -5.929113775071286, -1.1871766078924957, 
                    -4.124258811692425, 0.6969191559961637, -4.454022624935177, 
                    -2.352842192972056, -4.25145590011172])
    sigma = 1   # estimation on the data noise








.. GENERATED FROM PYTHON SOURCE LINES 87-89

And now lets plot the data.


.. GENERATED FROM PYTHON SOURCE LINES 89-97

.. code-block:: Python


    def plot_data(sigma=None):
        if(sigma is None):
            plt.scatter(data_x, data_y, color="lightcoral", label="observed data")
        else:
            plt.errorbar(data_x, data_y, yerr=sigma, fmt='.',color="lightcoral",ecolor='lightgrey',ms=10)
    plot_data()




.. image-sg:: /tutorials/generated/images/sphx_glr_linear_regression_001.png
   :alt: linear regression
   :srcset: /tutorials/generated/images/sphx_glr_linear_regression_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 102-139

Problem description
-------------------

To begin with, we will work with polynomial curves,

.. math:: y(x) = \sum_{j=0}^M m_j x^j\,.

Here, :math:`M` is the ‘order’ of the polynomial: if :math:`M=1` we have
a straight line with 2 parameters, if :math:`M=2` it will be a quadratic
with 3 parameters, and so on. The :math:`m_j, (j=0,\dots M)` are the
‘model coefficients’ that we seek to constrain from the data.

For this class of problem the forward operator takes the following form:

.. math:: \left(\begin{array}{c}y_0\\y_1\\\vdots\\y_N\end{array}\right) = \left(\begin{array}{ccc}1&x_0&x_0^2&x_0^3\\1&x_1&x_1^2&x_1^3\\\vdots&\vdots&\vdots\\1&x_N&x_N^2&x_N^3\end{array}\right)\left(\begin{array}{c}m_0\\m_1\\m_2\\m_3\end{array}\right)

This clearly has the required general form,
:math:`\mathbf{d} =G{\mathbf m}`.

where:

-  :math:`\textbf{d}` is the vector of data values,
   (:math:`y_0,y_1,\dots,y_N`);
-  :math:`\textbf{m}` is the vector of model parameters,
   (:math:`m_0,m_1,m_2`);
-  :math:`G` is the basis matrix (or design matrix) of this linear
   regression problem (also called the **Jacobian** matrix for this
   linear problem).

We have a set of noisy data values, :math:`y_i (i=0,\dots,N)`, measured
at known locations, :math:`x_i (i=0,\dots,N)`, and wish to find the best
fit degree 3 polynomial.

The function that generated our data is : :math:`y=-6-5x+2x^2+x^3`, and
we have added Gaussian random noise, :math:`{\cal N}(0,\sigma^2)`, with
:math:`\sigma=1.0`.


.. GENERATED FROM PYTHON SOURCE LINES 142-145

We now build the Jacobian/G matrix for this problem and define a forward
function which simply multiplies :math:`\mathbf m` by :math:`G`.


.. GENERATED FROM PYTHON SOURCE LINES 145-157

.. code-block:: Python


    nparams = 4 # Number of model parameters to be solved for

    def jacobian(x=data_x, n=nparams):
        return np.array([x**i for i in range(n)]).T

    def forward(model):
        return jacobian().dot(model)

    def Cd_inv(sigma=sigma, ndata=len(data_x)):
        return 1/sigma**2 * np.identity(ndata)








.. GENERATED FROM PYTHON SOURCE LINES 162-164

Define the true model for later.


.. GENERATED FROM PYTHON SOURCE LINES 164-171

.. code-block:: Python


    # True model for plotting
    x = np.linspace(-3.5,2.5)              # x values to plot
    true_model = np.array([-6, -5, 2, 1])  # we know it for this case which will be useful later for comparison.

    true_y = jacobian(x,4).dot(true_model) # y values for true curve








.. GENERATED FROM PYTHON SOURCE LINES 176-179

Now lets plot the data with the curve from the true polynomial
coefficients.


.. GENERATED FROM PYTHON SOURCE LINES 179-199

.. code-block:: Python


    # Some plotting utilities
    def plot_model(x,y, label, color=None):
        #x = np.linspace(-3.5,2.5)
        #y = jacobian(x).dot(model)
        plt.plot(x, y, color=color or "green", label=label)
        plt.xlabel("X")
        plt.ylabel("Y")
        plt.legend()

    def plot_models(models, label="Posterior samples", color="seagreen", alpha=0.1):
        x = np.linspace(-3.5,2.5)
        G = jacobian(x)
        plt.plot(x, G.dot(models[0]), color=color, label=label, alpha=alpha)
        for m in models:
            plt.plot(x, G.dot(m), color=color, alpha=alpha)
        plt.xlabel("X")
        plt.ylabel("Y")
        plt.legend()








.. GENERATED FROM PYTHON SOURCE LINES 201-205

.. code-block:: Python


    plot_data(sigma=sigma)
    plot_model(x,true_y, "true model")




.. image-sg:: /tutorials/generated/images/sphx_glr_linear_regression_002.png
   :alt: linear regression
   :srcset: /tutorials/generated/images/sphx_glr_linear_regression_002.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 210-213

Now we have the data and the forward model we can start to try and
estimate the coefficients of the polynomial from the data.


.. GENERATED FROM PYTHON SOURCE LINES 216-263

The structure of CoFI 
----------------------

In the workflow of ``cofi``, there are three main components:
``BaseProblem``, ``InversionOptions``, and ``Inversion``.

-  ``BaseProblem`` defines the inverse problem including any user
   supplied quantities such as data vector, number of model parameters
   and measure of fit between model predictions and data.

   .. code:: python

      inv_problem = BaseProblem()
      inv_problem.set_objective(some_function_here)
      inv_problem.set_jacobian(some_function_here)
      inv_problem.set_initial_model(a_starting_point) # if needed, e.g. we are solving a nonlinear problem by optimization

    

-  ``InversionOptions`` describes details about how one wants to run the
   inversion, including the backend tool and solver-specific parameters.
   It is based on the concept of a ``method`` and ``tool``.

   .. code:: python

      inv_options = InversionOptions()
      inv_options.suggest_solving_methods()
      inv_options.set_solving_method("matrix solvers")
      inv_options.suggest_tools()
      inv_options.set_tool("scipy.linalg.lstsq")
      inv_options.summary()

    

-  ``Inversion`` can be seen as an inversion engine that takes in the
   above two as information, and will produce an ``InversionResult``
   upon running.

   .. code:: python

      inv = Inversion(inv_problem, inv_options)
      result = inv.run()

Internally CoFI decides the nature of the problem from the quantities
set by the user and performs internal checks to ensure it has all that
it needs to solve a problem.


.. GENERATED FROM PYTHON SOURCE LINES 266-269

1. Linear system solver
-----------------------


.. GENERATED FROM PYTHON SOURCE LINES 269-272

.. code-block:: Python


    from cofi import BaseProblem, InversionOptions, Inversion








.. GENERATED FROM PYTHON SOURCE LINES 277-280

Step 1. Define CoFI ``BaseProblem``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


.. GENERATED FROM PYTHON SOURCE LINES 280-286

.. code-block:: Python


    inv_problem = BaseProblem()
    inv_problem.set_data(data_y)
    inv_problem.set_jacobian(jacobian())
    inv_problem.set_data_covariance_inv(Cd_inv())








.. GENERATED FROM PYTHON SOURCE LINES 291-294

Step 2. Define CoFI ``InversionOptions``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


.. GENERATED FROM PYTHON SOURCE LINES 294-297

.. code-block:: Python


    inv_options = InversionOptions()








.. GENERATED FROM PYTHON SOURCE LINES 302-305

Using the information supplied, we can ask CoFI to suggest some solving
methods.


.. GENERATED FROM PYTHON SOURCE LINES 305-308

.. code-block:: Python


    inv_options.suggest_solving_methods()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The following solving methods are supported:
    {'sampling', 'optimization', 'matrix solvers'}

    Use `suggest_tools()` to see a full list of backend tools for each method




.. GENERATED FROM PYTHON SOURCE LINES 313-315

We can ask CoFI to suggest some specific software tools as well.


.. GENERATED FROM PYTHON SOURCE LINES 315-318

.. code-block:: Python


    inv_options.suggest_tools()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Here's a complete list of inversion tools supported by CoFI (grouped by methods):
    {
        "optimization": [
            "scipy.optimize.minimize",
            "scipy.optimize.least_squares",
            "torch.optim",
            "cofi.border_collie_optimization"
        ],
        "matrix solvers": [
            "scipy.linalg.lstsq",
            "cofi.simple_newton"
        ],
        "sampling": [
            "emcee",
            "bayesbay",
            "neighpy"
        ]
    }




.. GENERATED FROM PYTHON SOURCE LINES 320-324

.. code-block:: Python


    inv_options.set_solving_method("matrix solvers") # lets decide to use a matrix solver.
    inv_options.summary()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    =============================
    Summary for inversion options
    =============================
    Solving method: matrix solvers
    Use `suggest_solving_methods()` to check available solving methods.
    -----------------------------
    Backend tool: `<class 'cofi.tools._scipy_lstsq.ScipyLstSq'> (by default)` - SciPy's wrapper function over LAPACK's linear least-squares solver, using 'gelsd', 'gelsy' (default), or 'gelss' as backend driver
    References: ['https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lstsq.html', 'https://www.netlib.org/lapack/lug/node27.html']
    Use `suggest_tools()` to check available backend tools.
    -----------------------------
    Solver-specific parameters: None set
    Use `suggest_solver_params()` to check required/optional solver-specific parameters.




.. GENERATED FROM PYTHON SOURCE LINES 326-330

.. code-block:: Python


    # below is optional, as this has already been the default tool under "linear least square"
    inv_options.set_tool("scipy.linalg.lstsq")








.. GENERATED FROM PYTHON SOURCE LINES 335-353

Step 3. Define CoFI ``Inversion`` and run
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Our choices so far have defined a linear parameter estimation problem
(without any regularization) to be solved within a least squares
framework. In this case the selection of a ``matrix solvers`` method
will mean we are calculating the standard least squares solution

.. math::


   m = (G^T C_d^{-1} G)^{-1} G^T C_d^{-1} d

and our choice of backend tool ``scipy.linalg.lstsq``, means that we
will employ scipy’s ``linalg`` package to perform the numerics.

Lets run CoFI.


.. GENERATED FROM PYTHON SOURCE LINES 353-357

.. code-block:: Python


    inv = Inversion(inv_problem, inv_options)
    inv_result = inv.run()








.. GENERATED FROM PYTHON SOURCE LINES 359-363

.. code-block:: Python


    print(f"The inversion result from `scipy.linalg.lstsq`: {inv_result.model}\n")
    inv_result.summary()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The inversion result from `scipy.linalg.lstsq`: [-5.71964359 -5.10903808  1.82553662  0.97472374]

    ============================
    Summary for inversion result
    ============================
    SUCCESS
    ----------------------------
    model: [-5.71964359 -5.10903808  1.82553662  0.97472374]
    sum_of_squared_residuals: []
    effective_rank: 4
    singular_values: [3765.51775745   69.19268194   16.27124488    3.85437889]
    model_covariance: [[ 0.19027447  0.05812534 -0.08168411 -0.02550866]
     [ 0.05812534  0.08673796 -0.03312809 -0.01812686]
     [-0.08168411 -0.03312809  0.05184851  0.01704165]
     [-0.02550866 -0.01812686  0.01704165  0.00676031]]




.. GENERATED FROM PYTHON SOURCE LINES 368-370

Lets plot the solution.


.. GENERATED FROM PYTHON SOURCE LINES 370-375

.. code-block:: Python


    plot_data()
    plot_model(x,jacobian(x).dot(inv_result.model), "linear system solver", color="seagreen")
    plot_model(x,true_y, "true model", color="darkorange")




.. image-sg:: /tutorials/generated/images/sphx_glr_linear_regression_003.png
   :alt: linear regression
   :srcset: /tutorials/generated/images/sphx_glr_linear_regression_003.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 380-405

2. Optimizer
------------

The same overdetermined linear problem,
:math:`\textbf{d} = G\textbf{m}`, with Gaussian data noise can also be
solved by minimising the squares of the residual of the linear
equations, e.g. :math:`\textbf{r}^T \textbf{C}_d^{-1}\textbf{r}` where
:math:`\textbf{r}=\textbf{d}-G\textbf{m}`. The above matrix solver
solution gives us the best data fitting model, but a direct optimisation
approach could also be used, say when the number of unknowns is large
and we do not wish, or are unable to provide the Jacobian function.

So we use a plain optimizer ``scipy.optimize.minimize`` to demonstrate
this ability.

.. raw:: html

   <!-- For this backend solver to run successfully, some additional information should be provided, otherwise
   you'll see an error to notify what additional information is required by the solver.

   There are several ways to provide the information needed to solve an inverse problem with 
   CoFI. In the example below we provide functions to calculate the data and the optional 
   regularisation. CoFI then generates the objective function for us based on the information 
   provided. The alternative to this would be to directly provide objective function to CoFI. -->


.. GENERATED FROM PYTHON SOURCE LINES 405-427

.. code-block:: Python


    ######## CoFI BaseProblem - provide additional information
    inv_problem.set_initial_model(np.ones(nparams))
    inv_problem.set_forward(forward)
    inv_problem.set_data_misfit("squared error")

    # inv_problem.set_objective(your_own_misfit_function)    # (optionally) if you'd like to define your own misfit
    # inv_problem.set_gradient(your_own_gradient_of_misfit_function)    # (optionally) if you'd like to define your own misfit gradient

    ######## CoFI InversionOptions - set a different tool
    inv_options_2 = InversionOptions()
    inv_options_2.set_tool("scipy.optimize.minimize")
    inv_options_2.set_params(method="Nelder-Mead")

    ######## CoFI Inversion - run it
    inv_2 = Inversion(inv_problem, inv_options_2)
    inv_result_2 = inv_2.run()

    ######## CoFI InversionResult - check result
    print(f"The inversion result from `scipy.optimize.minimize`: {inv_result_2.model}\n")
    inv_result_2.summary()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The inversion result from `scipy.optimize.minimize`: [-5.71967431 -5.10913992  1.82556456  0.9747426 ]

    ============================
    Summary for inversion result
    ============================
    SUCCESS
    ----------------------------
    fun: 14.961508008942793
    nit: 193
    nfev: 330
    status: 0
    message: Optimization terminated successfully.
    final_simplex: (array([[-5.71967431, -5.10913992,  1.82556456,  0.9747426 ],
           [-5.71958302, -5.10907158,  1.8255083 ,  0.97472628],
           [-5.71969118, -5.10911404,  1.82556388,  0.97474474],
           [-5.7197282 , -5.10917942,  1.82554925,  0.97474097],
           [-5.71960767, -5.10913354,  1.82551338,  0.97473478]]), array([14.96150801, 14.96150804, 14.96150805, 14.9615082 , 14.96150821]))
    model: [-5.71967431 -5.10913992  1.82556456  0.9747426 ]




.. GENERATED FROM PYTHON SOURCE LINES 429-434

.. code-block:: Python


    plot_data()
    plot_model(x,jacobian(x).dot(inv_result_2.model), "optimization solution", color="cornflowerblue")
    plot_model(x,true_y, "true model", color="darkorange")




.. image-sg:: /tutorials/generated/images/sphx_glr_linear_regression_004.png
   :alt: linear regression
   :srcset: /tutorials/generated/images/sphx_glr_linear_regression_004.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 439-441

--------------


.. GENERATED FROM PYTHON SOURCE LINES 444-473

Challenge: Change the polynomial degree
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Try and replace the 3rd order polynomial with a 1st order polynomial
(i.e. :math:`M=1`) by adding the required commands below. What does the
plot looks like?

|Upload to Jamboard 1|

Start from code below:

::

   inv_problem = BaseProblem()
   inv_problem.set_data(data_y)
   inv_problem.set_jacobian(jacobian(n=<CHANGE ME>))
   inv_problem.set_data_covariance_inv(Cd_inv())
   inv_options.set_solving_method("matrix solvers") # lets decide to use a matrix solver.
   inv = Inversion(inv_problem, inv_options)
   inv_result = inv.run()

   print("Inferred curve with n = <CHANGE ME> ")
   plot_data()
   plot_model(x,jacobian(x,n=<CHANGE ME>).dot(inv_result.model), "optimization solution", color="cornflowerblue")
   plot_model(x,true_y, "true model", color="darkorange")

.. |Upload to Jamboard 1| image:: https://img.shields.io/badge/Click%20&%20upload%20your%20results%20to-Jamboard-lightgrey?logo=jamboard&style=for-the-badge&color=fcbf49&labelColor=edede9
   :target: https://jamboard.google.com/d/1Fu_vIhWIlDl-gs9gzSPBNXLjzj2CsS70fLMDN8-7Sew/edit?usp=sharing


.. GENERATED FROM PYTHON SOURCE LINES 473-478

.. code-block:: Python


    # Copy the template above, Replace <CHANGE ME> with your answer










.. GENERATED FROM PYTHON SOURCE LINES 480-496

.. code-block:: Python


    #@title Solution

    inv_problem = BaseProblem()
    inv_problem.set_data(data_y)
    inv_problem.set_jacobian(jacobian(n=2))
    inv_problem.set_data_covariance_inv(Cd_inv())
    inv_options.set_solving_method("matrix solvers") # lets decide to use a matrix solver.
    inv = Inversion(inv_problem, inv_options)
    inv_result = inv.run()

    print("Inferred curve with n = 2 ")
    plot_data()
    plot_model(x,jacobian(x,n=2).dot(inv_result.model), "optimization solution", color="cornflowerblue")
    plot_model(x,true_y, "true model", color="darkorange")




.. image-sg:: /tutorials/generated/images/sphx_glr_linear_regression_005.png
   :alt: linear regression
   :srcset: /tutorials/generated/images/sphx_glr_linear_regression_005.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Inferred curve with n = 2 




.. GENERATED FROM PYTHON SOURCE LINES 501-503

--------------


.. GENERATED FROM PYTHON SOURCE LINES 506-509

3. Bayesian sampling
--------------------


.. GENERATED FROM PYTHON SOURCE LINES 512-535

Likelihood
~~~~~~~~~~

Since data errors follow a Gaussian in this example, we can define a
Likelihood function, :math:`p({\mathbf d}_{obs}| {\mathbf m})`.

.. math::


   p({\mathbf d}_{obs} | {\mathbf m}) \propto \exp \left\{- \frac{1}{2} ({\mathbf d}_{obs}-{\mathbf d}_{pred}({\mathbf m}))^T C_D^{-1} ({\mathbf d}_{obs}-{\mathbf d}_{pred}({\mathbf m})) \right\}

where :math:`{\mathbf d}_{obs}` represents the observed y values and
:math:`{\mathbf d}_{pred}({\mathbf m})` are those predicted by the
polynomial model :math:`({\mathbf m})`. The Likelihood is defined as the
probability of observing the data actually observed, given a model. In
practice we usually only need to evaluate the log of the Likelihood,
:math:`\log p({\mathbf d}_{obs} | {\mathbf m})`. To do so, we require
the inverse data covariance matrix describing the statistics of the
noise in the data, :math:`C_D^{-1}` . For this problem the data errors
are independent with identical standard deviation in noise for each
datum. Hence :math:`C_D^{-1} = \frac{1}{\sigma^2}I` where
:math:`\sigma=1`.


.. GENERATED FROM PYTHON SOURCE LINES 535-544

.. code-block:: Python


    sigma = 1.0                                     # common noise standard deviation
    Cdinv = np.eye(len(data_y))/(sigma**2)      # inverse data covariance matrix

    def log_likelihood(model):
        y_synthetics = forward(model)
        residual = data_y - y_synthetics
        return -0.5 * residual @ (Cdinv @ residual).T








.. GENERATED FROM PYTHON SOURCE LINES 549-552

Note that the user could specify **any appropriate Likelihood function**
of their choosing here.


.. GENERATED FROM PYTHON SOURCE LINES 555-579

Prior
~~~~~

Bayesian sampling requires a prior probability density function. A
common problem with polynomial coefficients as model parameters is that
it is not at all obvious what a prior should be. Here we choose a
uniform prior with specified bounds

.. math::


   \begin{align}
   p({\mathbf m}) &= \frac{1}{V},\quad  l_i \le m_i \le u_i, \quad (i=1,\dots,M)\\
   \\
            &= 0, \quad {\rm otherwise},
   \end{align}

where :math:`l_i` and :math:`u_i` are lower and upper bounds on the
:math:`i`\ th model coefficient.

Here use the uniform distribution with
:math:`{\mathbf l}^T = (-10.,-10.,-10.,-10.)`, and
:math:`{\mathbf u}^T = (10.,10.,10.,10.)`.


.. GENERATED FROM PYTHON SOURCE LINES 579-588

.. code-block:: Python


    m_lower_bound = np.ones(nparams) * (-10.)             # lower bound for uniform prior
    m_upper_bound = np.ones(nparams) * 10                 # upper bound for uniform prior

    def log_prior(model):    # uniform distribution
        for i in range(len(m_lower_bound)):
            if model[i] < m_lower_bound[i] or model[i] > m_upper_bound[i]: return -np.inf
        return 0.0 # model lies within bounds -> return log(1)








.. GENERATED FROM PYTHON SOURCE LINES 593-596

Note that the user could specify **any appropriate Prior PDF** of their
choosing here.


.. GENERATED FROM PYTHON SOURCE LINES 599-620

Bayesian sampling
~~~~~~~~~~~~~~~~~

In this aproach we sample a probability distribution rather than find a
single best fit solution. Bayes’ theorem tells us the the posterior
distribution is proportional to the Likelihood and the prior.

.. math:: p(\mathbf{m}|\mathbf{d}) = K p(\mathbf{d}|\mathbf{m})p(\mathbf{m})

where :math:`K` is some constant. Under the assumptions specified
:math:`p(\mathbf{m}|\mathbf{d})` gives a probability density of models
that are supported by the data. We seek to draw random samples from
:math:`p(\mathbf{m}|\mathbf{d})` over model space and then to make
inferences from the resulting ensemble of model parameters.

In this example we make use of *The Affine Invariant Markov chain Monte
Carlo (MCMC) Ensemble sampler* `Goodman and Weare
2010 <https://msp.org/camcos/2010/5-1/p04.xhtml>`__ to sample the
posterior distribution of the model. (See more details about
`emcee <https://emcee.readthedocs.io/en/stable/>`__).


.. GENERATED FROM PYTHON SOURCE LINES 623-630

Starting points for random walkers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Now we define some hyperparameters (e.g. the number of walkers and
steps), and initialise the starting positions of walkers. We start all
walkers in a small ball about a chosen point :math:`(0, 0, 0, 0)`.


.. GENERATED FROM PYTHON SOURCE LINES 630-636

.. code-block:: Python


    nwalkers = 32
    ndim = nparams
    nsteps = 10000
    walkers_start = np.zeros(nparams) + 1e-4 * np.random.randn(nwalkers, ndim)








.. GENERATED FROM PYTHON SOURCE LINES 641-644

Add the information and run with CoFI
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


.. GENERATED FROM PYTHON SOURCE LINES 644-663

.. code-block:: Python


    ######## CoFI BaseProblem - provide additional information
    inv_problem.set_log_prior(log_prior)
    inv_problem.set_log_likelihood(log_likelihood)
    inv_problem.set_model_shape(ndim)

    ######## CoFI InversionOptions - get a different tool
    inv_options_3 = InversionOptions()
    inv_options_3.set_tool("emcee")      # Here we use to Affine Invariant McMC sampler from Goodman and Weare (2010).
    inv_options_3.set_params(nwalkers=nwalkers, nsteps=nsteps, initial_state=walkers_start, progress=True)

    ######## CoFI Inversion - run it
    inv_3 = Inversion(inv_problem, inv_options_3)
    inv_result_3 = inv_3.run()

    ######## CoFI InversionResult - check result
    print(f"The inversion result from `emcee`:")
    inv_result_3.summary()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/10000 [00:00<?, ?it/s]      1%|          | 116/10000 [00:00<00:08, 1152.77it/s]      2%|▏         | 232/10000 [00:00<00:08, 1154.09it/s]      3%|▎         | 348/10000 [00:00<00:08, 1153.15it/s]      5%|▍         | 464/10000 [00:00<00:08, 1152.23it/s]      6%|▌         | 580/10000 [00:00<00:08, 1152.94it/s]      7%|▋         | 696/10000 [00:00<00:08, 1153.92it/s]      8%|▊         | 812/10000 [00:00<00:07, 1154.58it/s]      9%|▉         | 928/10000 [00:00<00:07, 1154.42it/s]     10%|█         | 1044/10000 [00:00<00:07, 1154.81it/s]     12%|█▏        | 1160/10000 [00:01<00:07, 1154.95it/s]     13%|█▎        | 1276/10000 [00:01<00:07, 1152.74it/s]     14%|█▍        | 1392/10000 [00:01<00:07, 1152.77it/s]     15%|█▌        | 1508/10000 [00:01<00:07, 1152.27it/s]     16%|█▌        | 1624/10000 [00:01<00:07, 1153.27it/s]     17%|█▋        | 1740/10000 [00:01<00:07, 1153.65it/s]     19%|█▊        | 1856/10000 [00:01<00:07, 1154.34it/s]     20%|█▉        | 1972/10000 [00:01<00:06, 1154.30it/s]     21%|██        | 2088/10000 [00:01<00:06, 1153.91it/s]     22%|██▏       | 2204/10000 [00:01<00:06, 1153.46it/s]     23%|██▎       | 2320/10000 [00:02<00:06, 1153.33it/s]     24%|██▍       | 2436/10000 [00:02<00:06, 1153.49it/s]     26%|██▌       | 2552/10000 [00:02<00:06, 1154.07it/s]     27%|██▋       | 2668/10000 [00:02<00:06, 1155.02it/s]     28%|██▊       | 2784/10000 [00:02<00:06, 1154.80it/s]     29%|██▉       | 2900/10000 [00:02<00:06, 1154.09it/s]     30%|███       | 3016/10000 [00:02<00:06, 1154.09it/s]     31%|███▏      | 3132/10000 [00:02<00:05, 1154.52it/s]     32%|███▏      | 3248/10000 [00:02<00:05, 1154.23it/s]     34%|███▎      | 3364/10000 [00:02<00:05, 1154.50it/s]     35%|███▍      | 3480/10000 [00:03<00:05, 1155.09it/s]     36%|███▌      | 3596/10000 [00:03<00:05, 1153.45it/s]     37%|███▋      | 3712/10000 [00:03<00:05, 1154.06it/s]     38%|███▊      | 3828/10000 [00:03<00:05, 1153.62it/s]     39%|███▉      | 3944/10000 [00:03<00:05, 1154.14it/s]     41%|████      | 4060/10000 [00:03<00:05, 1153.83it/s]     42%|████▏     | 4176/10000 [00:03<00:05, 1154.10it/s]     43%|████▎     | 4292/10000 [00:03<00:04, 1142.69it/s]     44%|████▍     | 4408/10000 [00:03<00:04, 1145.58it/s]     45%|████▌     | 4524/10000 [00:03<00:04, 1148.15it/s]     46%|████▋     | 4640/10000 [00:04<00:04, 1149.22it/s]     48%|████▊     | 4756/10000 [00:04<00:04, 1150.73it/s]     49%|████▊     | 4872/10000 [00:04<00:04, 1151.58it/s]     50%|████▉     | 4988/10000 [00:04<00:04, 1151.46it/s]     51%|█████     | 5104/10000 [00:04<00:04, 1151.73it/s]     52%|█████▏    | 5220/10000 [00:04<00:04, 1150.77it/s]     53%|█████▎    | 5336/10000 [00:04<00:04, 1151.82it/s]     55%|█████▍    | 5452/10000 [00:04<00:03, 1152.74it/s]     56%|█████▌    | 5568/10000 [00:04<00:03, 1153.38it/s]     57%|█████▋    | 5684/10000 [00:04<00:03, 1153.25it/s]     58%|█████▊    | 5800/10000 [00:05<00:03, 1152.88it/s]     59%|█████▉    | 5916/10000 [00:05<00:03, 1153.01it/s]     60%|██████    | 6032/10000 [00:05<00:03, 1152.99it/s]     61%|██████▏   | 6148/10000 [00:05<00:03, 1149.06it/s]     63%|██████▎   | 6264/10000 [00:05<00:03, 1150.26it/s]     64%|██████▍   | 6380/10000 [00:05<00:03, 1151.74it/s]     65%|██████▍   | 6496/10000 [00:05<00:03, 1152.57it/s]     66%|██████▌   | 6612/10000 [00:05<00:02, 1153.29it/s]     67%|██████▋   | 6728/10000 [00:05<00:02, 1153.36it/s]     68%|██████▊   | 6844/10000 [00:05<00:02, 1153.45it/s]     70%|██████▉   | 6960/10000 [00:06<00:02, 1152.89it/s]     71%|███████   | 7076/10000 [00:06<00:02, 1153.35it/s]     72%|███████▏  | 7192/10000 [00:06<00:02, 1154.02it/s]     73%|███████▎  | 7308/10000 [00:06<00:02, 1153.75it/s]     74%|███████▍  | 7424/10000 [00:06<00:02, 1153.85it/s]     75%|███████▌  | 7540/10000 [00:06<00:02, 1154.15it/s]     77%|███████▋  | 7656/10000 [00:06<00:02, 1153.86it/s]     78%|███████▊  | 7772/10000 [00:06<00:01, 1154.08it/s]     79%|███████▉  | 7888/10000 [00:06<00:01, 1154.07it/s]     80%|████████  | 8004/10000 [00:06<00:01, 1154.53it/s]     81%|████████  | 8120/10000 [00:07<00:01, 1153.80it/s]     82%|████████▏ | 8236/10000 [00:07<00:01, 1153.19it/s]     84%|████████▎ | 8352/10000 [00:07<00:01, 1153.35it/s]     85%|████████▍ | 8468/10000 [00:07<00:01, 1153.79it/s]     86%|████████▌ | 8584/10000 [00:07<00:01, 1154.72it/s]     87%|████████▋ | 8700/10000 [00:07<00:01, 1154.84it/s]     88%|████████▊ | 8816/10000 [00:07<00:01, 1154.93it/s]     89%|████████▉ | 8932/10000 [00:07<00:00, 1155.69it/s]     90%|█████████ | 9048/10000 [00:07<00:00, 1155.54it/s]     92%|█████████▏| 9164/10000 [00:07<00:00, 1155.69it/s]     93%|█████████▎| 9280/10000 [00:08<00:00, 1154.70it/s]     94%|█████████▍| 9396/10000 [00:08<00:00, 1153.92it/s]     95%|█████████▌| 9512/10000 [00:08<00:00, 1153.47it/s]     96%|█████████▋| 9628/10000 [00:08<00:00, 1153.29it/s]     97%|█████████▋| 9744/10000 [00:08<00:00, 1153.35it/s]     99%|█████████▊| 9860/10000 [00:08<00:00, 1147.60it/s]    100%|█████████▉| 9976/10000 [00:08<00:00, 1149.54it/s]    100%|██████████| 10000/10000 [00:08<00:00, 1152.93it/s]
    The inversion result from `emcee`:
    ============================
    Summary for inversion result
    ============================
    SUCCESS
    ----------------------------
    sampler: <emcee.ensemble.EnsembleSampler object>
    blob_names: ['log_likelihood', 'log_prior']




.. GENERATED FROM PYTHON SOURCE LINES 668-679

Post-sampling analysis
~~~~~~~~~~~~~~~~~~~~~~

By default the raw sampler resulting object is attached to ``cofi``\ ’s
inversion result.

Optionally, you can convert that into an ``arviz`` data structure to
have access to a range of analysis functions. (See more details in
`arviz
documentation <https://python.arviz.org/en/latest/index.html>`__).


.. GENERATED FROM PYTHON SOURCE LINES 679-688

.. code-block:: Python


    import arviz as az

    labels = ["m0", "m1", "m2","m3"]

    sampler = inv_result_3.sampler
    az_idata = az.from_emcee(sampler, var_names=labels)
    # az_idata = inv_result_3.to_arviz()      # alternatively








.. GENERATED FROM PYTHON SOURCE LINES 690-693

.. code-block:: Python


    az_idata.get("posterior")






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div><svg style="position: absolute; width: 0; height: 0; overflow: hidden">
    <defs>
    <symbol id="icon-database" viewBox="0 0 32 32">
    <path d="M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z"></path>
    <path d="M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z"></path>
    <path d="M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z"></path>
    </symbol>
    <symbol id="icon-file-text2" viewBox="0 0 32 32">
    <path d="M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z"></path>
    <path d="M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z"></path>
    <path d="M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z"></path>
    <path d="M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z"></path>
    </symbol>
    </defs>
    </svg>
    <style>/* CSS stylesheet for displaying xarray objects in jupyterlab.
     *
     */

    :root {
      --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));
      --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));
      --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));
      --xr-border-color: var(--jp-border-color2, #e0e0e0);
      --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);
      --xr-background-color: var(--jp-layout-color0, white);
      --xr-background-color-row-even: var(--jp-layout-color1, white);
      --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);
    }

    html[theme=dark],
    body[data-theme=dark],
    body.vscode-dark {
      --xr-font-color0: rgba(255, 255, 255, 1);
      --xr-font-color2: rgba(255, 255, 255, 0.54);
      --xr-font-color3: rgba(255, 255, 255, 0.38);
      --xr-border-color: #1F1F1F;
      --xr-disabled-color: #515151;
      --xr-background-color: #111111;
      --xr-background-color-row-even: #111111;
      --xr-background-color-row-odd: #313131;
    }

    .xr-wrap {
      display: block !important;
      min-width: 300px;
      max-width: 700px;
    }

    .xr-text-repr-fallback {
      /* fallback to plain text repr when CSS is not injected (untrusted notebook) */
      display: none;
    }

    .xr-header {
      padding-top: 6px;
      padding-bottom: 6px;
      margin-bottom: 4px;
      border-bottom: solid 1px var(--xr-border-color);
    }

    .xr-header > div,
    .xr-header > ul {
      display: inline;
      margin-top: 0;
      margin-bottom: 0;
    }

    .xr-obj-type,
    .xr-array-name {
      margin-left: 2px;
      margin-right: 10px;
    }

    .xr-obj-type {
      color: var(--xr-font-color2);
    }

    .xr-sections {
      padding-left: 0 !important;
      display: grid;
      grid-template-columns: 150px auto auto 1fr 20px 20px;
    }

    .xr-section-item {
      display: contents;
    }

    .xr-section-item input {
      display: none;
    }

    .xr-section-item input + label {
      color: var(--xr-disabled-color);
    }

    .xr-section-item input:enabled + label {
      cursor: pointer;
      color: var(--xr-font-color2);
    }

    .xr-section-item input:enabled + label:hover {
      color: var(--xr-font-color0);
    }

    .xr-section-summary {
      grid-column: 1;
      color: var(--xr-font-color2);
      font-weight: 500;
    }

    .xr-section-summary > span {
      display: inline-block;
      padding-left: 0.5em;
    }

    .xr-section-summary-in:disabled + label {
      color: var(--xr-font-color2);
    }

    .xr-section-summary-in + label:before {
      display: inline-block;
      content: '►';
      font-size: 11px;
      width: 15px;
      text-align: center;
    }

    .xr-section-summary-in:disabled + label:before {
      color: var(--xr-disabled-color);
    }

    .xr-section-summary-in:checked + label:before {
      content: '▼';
    }

    .xr-section-summary-in:checked + label > span {
      display: none;
    }

    .xr-section-summary,
    .xr-section-inline-details {
      padding-top: 4px;
      padding-bottom: 4px;
    }

    .xr-section-inline-details {
      grid-column: 2 / -1;
    }

    .xr-section-details {
      display: none;
      grid-column: 1 / -1;
      margin-bottom: 5px;
    }

    .xr-section-summary-in:checked ~ .xr-section-details {
      display: contents;
    }

    .xr-array-wrap {
      grid-column: 1 / -1;
      display: grid;
      grid-template-columns: 20px auto;
    }

    .xr-array-wrap > label {
      grid-column: 1;
      vertical-align: top;
    }

    .xr-preview {
      color: var(--xr-font-color3);
    }

    .xr-array-preview,
    .xr-array-data {
      padding: 0 5px !important;
      grid-column: 2;
    }

    .xr-array-data,
    .xr-array-in:checked ~ .xr-array-preview {
      display: none;
    }

    .xr-array-in:checked ~ .xr-array-data,
    .xr-array-preview {
      display: inline-block;
    }

    .xr-dim-list {
      display: inline-block !important;
      list-style: none;
      padding: 0 !important;
      margin: 0;
    }

    .xr-dim-list li {
      display: inline-block;
      padding: 0;
      margin: 0;
    }

    .xr-dim-list:before {
      content: '(';
    }

    .xr-dim-list:after {
      content: ')';
    }

    .xr-dim-list li:not(:last-child):after {
      content: ',';
      padding-right: 5px;
    }

    .xr-has-index {
      font-weight: bold;
    }

    .xr-var-list,
    .xr-var-item {
      display: contents;
    }

    .xr-var-item > div,
    .xr-var-item label,
    .xr-var-item > .xr-var-name span {
      background-color: var(--xr-background-color-row-even);
      margin-bottom: 0;
    }

    .xr-var-item > .xr-var-name:hover span {
      padding-right: 5px;
    }

    .xr-var-list > li:nth-child(odd) > div,
    .xr-var-list > li:nth-child(odd) > label,
    .xr-var-list > li:nth-child(odd) > .xr-var-name span {
      background-color: var(--xr-background-color-row-odd);
    }

    .xr-var-name {
      grid-column: 1;
    }

    .xr-var-dims {
      grid-column: 2;
    }

    .xr-var-dtype {
      grid-column: 3;
      text-align: right;
      color: var(--xr-font-color2);
    }

    .xr-var-preview {
      grid-column: 4;
    }

    .xr-index-preview {
      grid-column: 2 / 5;
      color: var(--xr-font-color2);
    }

    .xr-var-name,
    .xr-var-dims,
    .xr-var-dtype,
    .xr-preview,
    .xr-attrs dt {
      white-space: nowrap;
      overflow: hidden;
      text-overflow: ellipsis;
      padding-right: 10px;
    }

    .xr-var-name:hover,
    .xr-var-dims:hover,
    .xr-var-dtype:hover,
    .xr-attrs dt:hover {
      overflow: visible;
      width: auto;
      z-index: 1;
    }

    .xr-var-attrs,
    .xr-var-data,
    .xr-index-data {
      display: none;
      background-color: var(--xr-background-color) !important;
      padding-bottom: 5px !important;
    }

    .xr-var-attrs-in:checked ~ .xr-var-attrs,
    .xr-var-data-in:checked ~ .xr-var-data,
    .xr-index-data-in:checked ~ .xr-index-data {
      display: block;
    }

    .xr-var-data > table {
      float: right;
    }

    .xr-var-name span,
    .xr-var-data,
    .xr-index-name div,
    .xr-index-data,
    .xr-attrs {
      padding-left: 25px !important;
    }

    .xr-attrs,
    .xr-var-attrs,
    .xr-var-data,
    .xr-index-data {
      grid-column: 1 / -1;
    }

    dl.xr-attrs {
      padding: 0;
      margin: 0;
      display: grid;
      grid-template-columns: 125px auto;
    }

    .xr-attrs dt,
    .xr-attrs dd {
      padding: 0;
      margin: 0;
      float: left;
      padding-right: 10px;
      width: auto;
    }

    .xr-attrs dt {
      font-weight: normal;
      grid-column: 1;
    }

    .xr-attrs dt:hover span {
      display: inline-block;
      background: var(--xr-background-color);
      padding-right: 10px;
    }

    .xr-attrs dd {
      grid-column: 2;
      white-space: pre-wrap;
      word-break: break-all;
    }

    .xr-icon-database,
    .xr-icon-file-text2,
    .xr-no-icon {
      display: inline-block;
      vertical-align: middle;
      width: 1em;
      height: 1.5em !important;
      stroke-width: 0;
      stroke: currentColor;
      fill: currentColor;
    }
    </style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt;
    Dimensions:  (chain: 32, draw: 10000)
    Coordinates:
      * chain    (chain) int64 0 1 2 3 4 5 6 7 8 9 ... 22 23 24 25 26 27 28 29 30 31
      * draw     (draw) int64 0 1 2 3 4 5 6 7 ... 9993 9994 9995 9996 9997 9998 9999
    Data variables:
        m0       (chain, draw) float64 4.696e-05 4.324e-05 ... -6.243 -6.211
        m1       (chain, draw) float64 -1.484e-05 -1.471e-05 ... -5.121 -5.189
        m2       (chain, draw) float64 6.432e-05 5.766e-05 0.0003288 ... 2.105 2.102
        m3       (chain, draw) float64 0.0001544 0.0001576 0.0005567 ... 1.026 1.04
    Attributes:
        created_at:                 2024-04-17T06:41:48.913804
        arviz_version:              0.17.0
        inference_library:          emcee
        inference_library_version:  3.1.4</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-c5249436-ffc4-4af1-903c-4390ffe57179' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-c5249436-ffc4-4af1-903c-4390ffe57179' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>chain</span>: 32</li><li><span class='xr-has-index'>draw</span>: 10000</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-10f7aec9-f90a-4b79-842f-93a8acebf9df' class='xr-section-summary-in' type='checkbox'  checked><label for='section-10f7aec9-f90a-4b79-842f-93a8acebf9df' class='xr-section-summary' >Coordinates: <span>(2)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>chain</span></div><div class='xr-var-dims'>(chain)</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>0 1 2 3 4 5 6 ... 26 27 28 29 30 31</div><input id='attrs-f518732d-095f-4891-b6f2-998ce5cef7f1' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-f518732d-095f-4891-b6f2-998ce5cef7f1' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-13747c95-d8ef-4bbe-aac0-a572b81d696f' class='xr-var-data-in' type='checkbox'><label for='data-13747c95-d8ef-4bbe-aac0-a572b81d696f' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
           18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>draw</span></div><div class='xr-var-dims'>(draw)</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>0 1 2 3 4 ... 9996 9997 9998 9999</div><input id='attrs-5545f908-0643-49e4-9c87-bce6e302c7a4' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-5545f908-0643-49e4-9c87-bce6e302c7a4' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-93c4394d-776c-4b0b-a274-9a67a3a4a8b3' class='xr-var-data-in' type='checkbox'><label for='data-93c4394d-776c-4b0b-a274-9a67a3a4a8b3' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([   0,    1,    2, ..., 9997, 9998, 9999])</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-6163419f-ce84-4747-b14d-33c583bd3a00' class='xr-section-summary-in' type='checkbox'  checked><label for='section-6163419f-ce84-4747-b14d-33c583bd3a00' class='xr-section-summary' >Data variables: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>m0</span></div><div class='xr-var-dims'>(chain, draw)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>4.696e-05 4.324e-05 ... -6.211</div><input id='attrs-ff45acb8-603d-4595-b259-48ea19be57ab' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-ff45acb8-603d-4595-b259-48ea19be57ab' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-417b720f-717a-4224-8695-0a85bc65162c' class='xr-var-data-in' type='checkbox'><label for='data-417b720f-717a-4224-8695-0a85bc65162c' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([[ 4.69621369e-05,  4.32447115e-05, -1.66106827e-04, ...,
            -6.00425994e+00, -6.00506574e+00, -6.09138800e+00],
           [-3.47949875e-05, -3.83495998e-05, -3.12731408e-05, ...,
            -5.31716533e+00, -5.44423325e+00, -5.39279831e+00],
           [-1.51789861e-04, -9.99824205e-05,  2.39828695e-05, ...,
            -6.33625405e+00, -6.33625405e+00, -5.95247263e+00],
           ...,
           [ 7.60861000e-06,  6.81590656e-06,  6.81590656e-06, ...,
            -5.53089703e+00, -5.53089703e+00, -5.44182574e+00],
           [ 2.00843973e-04,  3.51867326e-04,  3.51867326e-04, ...,
            -5.88423415e+00, -5.88423415e+00, -5.81652626e+00],
           [ 1.05025143e-04,  2.96183691e-05,  4.20439019e-05, ...,
            -6.48819770e+00, -6.24266197e+00, -6.21136768e+00]])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>m1</span></div><div class='xr-var-dims'>(chain, draw)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-1.484e-05 -1.471e-05 ... -5.189</div><input id='attrs-c6077124-23b2-4ea4-ad71-41ac85cc49c1' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-c6077124-23b2-4ea4-ad71-41ac85cc49c1' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-1c56c1b6-7ddc-4c08-9ecc-4160cfaa5731' class='xr-var-data-in' type='checkbox'><label for='data-1c56c1b6-7ddc-4c08-9ecc-4160cfaa5731' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([[-1.48406903e-05, -1.47134825e-05,  3.49152074e-05, ...,
            -5.59625291e+00, -5.60119586e+00, -5.45227691e+00],
           [-3.10270145e-05, -4.11498590e-05, -1.46954992e-04, ...,
            -5.35659407e+00, -5.40651541e+00, -5.53621139e+00],
           [-1.09300008e-04, -1.79388137e-06,  1.49112452e-04, ...,
            -5.17469777e+00, -5.17469777e+00, -5.24114261e+00],
           ...,
           [-1.41373004e-04, -1.38524879e-04, -1.38524879e-04, ...,
            -5.17578055e+00, -5.17578055e+00, -5.18266661e+00],
           [-8.12575779e-05, -1.33635910e-04, -1.33635910e-04, ...,
            -4.85999288e+00, -4.85999288e+00, -4.82054681e+00],
           [ 5.46033440e-04,  7.53662755e-04,  1.00966175e-03, ...,
            -5.10177208e+00, -5.12075433e+00, -5.18933694e+00]])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>m2</span></div><div class='xr-var-dims'>(chain, draw)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>6.432e-05 5.766e-05 ... 2.105 2.102</div><input id='attrs-6a852113-f876-4e30-94b2-0ce485a4ba9b' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-6a852113-f876-4e30-94b2-0ce485a4ba9b' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-a1da8345-f3c9-400c-a202-17c2b74c0f67' class='xr-var-data-in' type='checkbox'><label for='data-a1da8345-f3c9-400c-a202-17c2b74c0f67' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([[ 6.43235990e-05,  5.76612692e-05,  3.28784442e-04, ...,
             2.29065099e+00,  2.29375201e+00,  2.09089797e+00],
           [ 1.44452179e-04,  2.90424376e-04,  4.12538935e-04, ...,
             1.75699073e+00,  1.80003161e+00,  1.80587662e+00],
           [ 5.97551492e-06, -5.72990899e-05, -1.04812616e-04, ...,
             1.87314304e+00,  1.87314304e+00,  1.87629362e+00],
           ...,
           [ 1.70350433e-04,  1.63861242e-04,  1.63861242e-04, ...,
             1.84908524e+00,  1.84908524e+00,  1.81701487e+00],
           [ 2.11092575e-04,  7.36987895e-04,  7.36987895e-04, ...,
             1.82874996e+00,  1.82874996e+00,  1.70406111e+00],
           [-3.43469076e-04, -6.73533305e-04, -9.88862833e-04, ...,
             2.19376408e+00,  2.10535824e+00,  2.10236682e+00]])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>m3</span></div><div class='xr-var-dims'>(chain, draw)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>0.0001544 0.0001576 ... 1.026 1.04</div><input id='attrs-4b5fd761-0dc5-4be2-93f8-35605c52864a' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-4b5fd761-0dc5-4be2-93f8-35605c52864a' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-8aebebc1-59bc-4f38-a5f5-6ab7eefbffb5' class='xr-var-data-in' type='checkbox'><label for='data-8aebebc1-59bc-4f38-a5f5-6ab7eefbffb5' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([[ 1.54398590e-04,  1.57577824e-04,  5.56734446e-04, ...,
             1.17967808e+00,  1.18130529e+00,  1.09693799e+00],
           [ 6.66969946e-05,  1.59940716e-05, -6.39806635e-05, ...,
             1.01210574e+00,  1.01969811e+00,  1.03113708e+00],
           [-3.65914587e-05, -9.54517376e-05, -9.63893480e-05, ...,
             9.96500154e-01,  9.96500154e-01,  1.00704895e+00],
           ...,
           [ 1.23757522e-04,  1.24552189e-04,  1.24552189e-04, ...,
             9.69310878e-01,  9.69310878e-01,  9.62269020e-01],
           [-1.94805940e-04, -3.70217299e-04, -3.70217299e-04, ...,
             9.37303242e-01,  9.37303242e-01,  8.85069357e-01],
           [-2.20285212e-04, -1.74483093e-04, -1.59264525e-04, ...,
             1.04499380e+00,  1.02558207e+00,  1.04034358e+00]])</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-98575de5-4db3-4f5f-8f19-a48a6ebf848f' class='xr-section-summary-in' type='checkbox'  ><label for='section-98575de5-4db3-4f5f-8f19-a48a6ebf848f' class='xr-section-summary' >Indexes: <span>(2)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>chain</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-46f1b173-0f8c-403d-96f2-22e207662b56' class='xr-index-data-in' type='checkbox'/><label for='index-46f1b173-0f8c-403d-96f2-22e207662b56' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
           18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],
          dtype=&#x27;int64&#x27;, name=&#x27;chain&#x27;))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>draw</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-bb3afaf5-c9c9-49bf-8b4e-8aceb4058674' class='xr-index-data-in' type='checkbox'/><label for='index-bb3afaf5-c9c9-49bf-8b4e-8aceb4058674' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,
           ...
           9990, 9991, 9992, 9993, 9994, 9995, 9996, 9997, 9998, 9999],
          dtype=&#x27;int64&#x27;, name=&#x27;draw&#x27;, length=10000))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-9ba873ab-844a-49cb-afbd-2186ccbf77c7' class='xr-section-summary-in' type='checkbox'  checked><label for='section-9ba873ab-844a-49cb-afbd-2186ccbf77c7' class='xr-section-summary' >Attributes: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>created_at :</span></dt><dd>2024-04-17T06:41:48.913804</dd><dt><span>arviz_version :</span></dt><dd>0.17.0</dd><dt><span>inference_library :</span></dt><dd>emcee</dd><dt><span>inference_library_version :</span></dt><dd>3.1.4</dd></dl></div></li></ul></div></div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 695-709

.. code-block:: Python


    # a standard `trace` plot
    axes = az.plot_trace(az_idata, backend_kwargs={"constrained_layout":True}); 

    # add legends
    for i, axes_pair in enumerate(axes):
        ax1 = axes_pair[0]
        ax2 = axes_pair[1]
        ax1.axvline(true_model[i], linestyle='dotted', color='red')
        ax1.set_xlabel("parameter value")
        ax1.set_ylabel("density value")
        ax2.set_xlabel("number of iterations")
        ax2.set_ylabel("parameter value")




.. image-sg:: /tutorials/generated/images/sphx_glr_linear_regression_006.png
   :alt: m0, m0, m1, m1, m2, m2, m3, m3
   :srcset: /tutorials/generated/images/sphx_glr_linear_regression_006.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 711-715

.. code-block:: Python


    tau = sampler.get_autocorr_time()
    print(f"autocorrelation time: {tau}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    autocorrelation time: [ 87.58993647 110.95951699  68.36367543  89.68019852]




.. GENERATED FROM PYTHON SOURCE LINES 717-742

.. code-block:: Python


    # a Corner plot

    fig, axes = plt.subplots(nparams, nparams, figsize=(12,8))

    if(False): # if we are plotting the model ensemble use this
        az.plot_pair(
            az_idata.sel(draw=slice(300,None)), 
            marginals=True, 
            reference_values=dict(zip([f"m{i}" for i in range(4)], true_model.tolist())),
            ax=axes,
        );
    else: # if we wish to plot a kernel density plot then use this option
        az.plot_pair(
            az_idata.sel(draw=slice(300,None)), 
            marginals=True, 
            reference_values=dict(zip([f"m{i}" for i in range(4)], true_model.tolist())),
            kind="kde",
            kde_kwargs={
                "hdi_probs": [0.3, 0.6, 0.9],  # Plot 30%, 60% and 90% HDI contours
                "contourf_kwargs": {"cmap": "Blues"},
            },
            ax=axes,
        );




.. image-sg:: /tutorials/generated/images/sphx_glr_linear_regression_007.png
   :alt: linear regression
   :srcset: /tutorials/generated/images/sphx_glr_linear_regression_007.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 747-750

Now we plot the predicted curves for the posterior ensemble of
solutions.


.. GENERATED FROM PYTHON SOURCE LINES 750-758

.. code-block:: Python


    flat_samples = sampler.get_chain(discard=300, thin=30, flat=True)
    inds = np.random.randint(len(flat_samples), size=100) # get a random selection from posterior ensemble

    plot_data()
    plot_models(flat_samples[inds])
    plot_model(x,true_y, "True model", color="darkorange")




.. image-sg:: /tutorials/generated/images/sphx_glr_linear_regression_008.png
   :alt: linear regression
   :srcset: /tutorials/generated/images/sphx_glr_linear_regression_008.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 763-766

Expected values, credible intervals and model covariance matrix from the ensemble
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


.. GENERATED FROM PYTHON SOURCE LINES 766-772

.. code-block:: Python


    print("\n Expected value and 95% credible intervals ")
    for i in range(ndim):
        mcmc = np.percentile(flat_samples[:, i], [5, 50, 95])
        print(" {} {:7.3f} [{:7.3f}, {:7.3f}]".format(labels[i],mcmc[1],mcmc[0],mcmc[2]))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


     Expected value and 95% credible intervals 
     m0  -5.713 [ -6.436,  -4.994]
     m1  -5.112 [ -5.601,  -4.631]
     m2   1.822 [  1.445,   2.193]
     m3   0.974 [  0.839,   1.108]




.. GENERATED FROM PYTHON SOURCE LINES 774-782

.. code-block:: Python


    CMpost = np.cov(flat_samples.T)
    CM_std= np.std(flat_samples,axis=0)
    print('Posterior model covariance matrix\n',CMpost)
    print('\n Posterior estimate of model standard deviations in each parameter')
    for i in range(ndim):
        print("    {} {:7.4f}".format(labels[i],CM_std[i]))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Posterior model covariance matrix
     [[ 0.19127786  0.05764904 -0.08160666 -0.02533072]
     [ 0.05764904  0.08724585 -0.03311225 -0.01816461]
     [-0.08160666 -0.03311225  0.05134851  0.01683258]
     [-0.02533072 -0.01816461  0.01683258  0.00668753]]

     Posterior estimate of model standard deviations in each parameter
        m0  0.4373
        m1  0.2954
        m2  0.2266
        m3  0.0818




.. GENERATED FROM PYTHON SOURCE LINES 787-789

--------------


.. GENERATED FROM PYTHON SOURCE LINES 792-795

Challenge: Change the prior model bounds
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


.. GENERATED FROM PYTHON SOURCE LINES 798-848

Replace the previous prior bounds to new values

The original uniform bounds had

:math:`{\mathbf l}^T = (-10.,-10.,-10.,-10.)`, and
:math:`{\mathbf u}^T = (10.,10.,10.,10.)`.

Lets replace with

:math:`{\mathbf l}^T = (-1.,-10.,-10.,-10.)`, and
:math:`{\mathbf u}^T = (2.,10.,10.,10.)`.

We have only changed the bounds of the first parameter. However since
the true value of constant term was 6, these bounds are now inconsistent
with the true model.

What does this do to the posterior distribution?

|Upload to Jamboard 2|

Start from the code template below:

::

   m_lower_bound = <CHANGE ME>             # lower bound for uniform prior
   m_upper_bound = <CHANGE ME>             # upper bound for uniform prior

   def log_prior(model):    # uniform distribution
       for i in range(len(m_lower_bound)):
           if model[i] < m_lower_bound[i] or model[i] > m_upper_bound[i]: return -np.inf
       return 0.0 # model lies within bounds -> return log(1)

   ######## CoFI BaseProblem - update information
   inv_problem.set_log_prior(log_prior)

   ######## CoFI Inversion - run it
   inv_4 = Inversion(inv_problem, inv_options_3)
   inv_result_4 = inv_4.run()

   flat_samples = inv_result_4.sampler.get_chain(discard=300, thin=30, flat=True)
   inds = np.random.randint(len(flat_samples), size=100) # get a random selection from posterior ensemble

   print("Resulting samples with prior model lower bounds of <CHANGE ME>, upper bounds of <CHANGE ME>")
   plot_data()
   plot_models(flat_samples[inds])
   plot_model(x, true_y, "True model", color="darkorange")

.. |Upload to Jamboard 2| image:: https://img.shields.io/badge/Click%20&%20upload%20your%20results%20to-Jamboard-lightgrey?logo=jamboard&style=for-the-badge&color=fcbf49&labelColor=edede9
   :target: https://jamboard.google.com/d/1h_O8PNuHzpyH2zQUraqiMT4SQR0TMhUmiZzFn_HMZl4/edit?usp=sharing


.. GENERATED FROM PYTHON SOURCE LINES 848-853

.. code-block:: Python


    # Copy the template above, Replace <CHANGE ME> with your answer










.. GENERATED FROM PYTHON SOURCE LINES 855-881

.. code-block:: Python


    #@title Solution

    m_lower_bound = np.array([-1,-10,-10,-10])             # lower bound for uniform prior
    m_upper_bound = np.array([2,10,10,10])                 # upper bound for uniform prior

    def log_prior(model):    # uniform distribution
        for i in range(len(m_lower_bound)):
            if model[i] < m_lower_bound[i] or model[i] > m_upper_bound[i]: return -np.inf
        return 0.0 # model lies within bounds -> return log(1)

    ######## CoFI BaseProblem - update information
    inv_problem.set_log_prior(log_prior)

    ######## CoFI Inversion - run it
    inv_4 = Inversion(inv_problem, inv_options_3)
    inv_result_4 = inv_4.run()

    flat_samples = inv_result_4.sampler.get_chain(discard=300, thin=30, flat=True)
    inds = np.random.randint(len(flat_samples), size=100) # get a random selection from posterior ensemble

    print("Resulting samples with prior model lower bounds of [-1,-10,-10,-10], upper bounds of [2,10,10,10]")
    plot_data()
    plot_models(flat_samples[inds])
    plot_model(x, true_y, "True model", color="darkorange")




.. image-sg:: /tutorials/generated/images/sphx_glr_linear_regression_009.png
   :alt: linear regression
   :srcset: /tutorials/generated/images/sphx_glr_linear_regression_009.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/10000 [00:00<?, ?it/s]      1%|          | 117/10000 [00:00<00:08, 1163.76it/s]      2%|▏         | 238/10000 [00:00<00:08, 1189.64it/s]      4%|▎         | 362/10000 [00:00<00:07, 1208.31it/s]      5%|▍         | 485/10000 [00:00<00:07, 1214.27it/s]      6%|▌         | 608/10000 [00:00<00:07, 1218.63it/s]      7%|▋         | 732/10000 [00:00<00:07, 1225.21it/s]      9%|▊         | 855/10000 [00:00<00:07, 1224.43it/s]     10%|▉         | 979/10000 [00:00<00:07, 1226.85it/s]     11%|█         | 1103/10000 [00:00<00:07, 1229.22it/s]     12%|█▏        | 1226/10000 [00:01<00:07, 1226.40it/s]     13%|█▎        | 1349/10000 [00:01<00:07, 1222.96it/s]     15%|█▍        | 1473/10000 [00:01<00:06, 1225.46it/s]     16%|█▌        | 1597/10000 [00:01<00:06, 1228.00it/s]     17%|█▋        | 1720/10000 [00:01<00:06, 1223.67it/s]     18%|█▊        | 1844/10000 [00:01<00:06, 1226.36it/s]     20%|█▉        | 1968/10000 [00:01<00:06, 1227.50it/s]     21%|██        | 2091/10000 [00:01<00:06, 1226.61it/s]     22%|██▏       | 2215/10000 [00:01<00:06, 1228.11it/s]     23%|██▎       | 2338/10000 [00:01<00:06, 1226.50it/s]     25%|██▍       | 2461/10000 [00:02<00:06, 1223.21it/s]     26%|██▌       | 2585/10000 [00:02<00:06, 1227.52it/s]     27%|██▋       | 2708/10000 [00:02<00:05, 1225.42it/s]     28%|██▊       | 2831/10000 [00:02<00:05, 1224.93it/s]     30%|██▉       | 2954/10000 [00:02<00:05, 1223.27it/s]     31%|███       | 3077/10000 [00:02<00:05, 1220.66it/s]     32%|███▏      | 3200/10000 [00:02<00:05, 1222.98it/s]     33%|███▎      | 3323/10000 [00:02<00:05, 1220.57it/s]     34%|███▍      | 3446/10000 [00:02<00:05, 1222.56it/s]     36%|███▌      | 3569/10000 [00:02<00:05, 1221.89it/s]     37%|███▋      | 3692/10000 [00:03<00:05, 1214.62it/s]     38%|███▊      | 3815/10000 [00:03<00:05, 1217.09it/s]     39%|███▉      | 3938/10000 [00:03<00:04, 1220.24it/s]     41%|████      | 4061/10000 [00:03<00:04, 1219.42it/s]     42%|████▏     | 4184/10000 [00:03<00:04, 1222.09it/s]     43%|████▎     | 4307/10000 [00:03<00:04, 1220.74it/s]     44%|████▍     | 4431/10000 [00:03<00:04, 1225.48it/s]     46%|████▌     | 4554/10000 [00:03<00:04, 1224.83it/s]     47%|████▋     | 4677/10000 [00:03<00:04, 1221.85it/s]     48%|████▊     | 4801/10000 [00:03<00:04, 1226.88it/s]     49%|████▉     | 4924/10000 [00:04<00:04, 1226.13it/s]     50%|█████     | 5048/10000 [00:04<00:04, 1227.83it/s]     52%|█████▏    | 5171/10000 [00:04<00:03, 1228.38it/s]     53%|█████▎    | 5294/10000 [00:04<00:03, 1227.04it/s]     54%|█████▍    | 5417/10000 [00:04<00:03, 1227.26it/s]     55%|█████▌    | 5540/10000 [00:04<00:03, 1226.66it/s]     57%|█████▋    | 5663/10000 [00:04<00:03, 1226.48it/s]     58%|█████▊    | 5786/10000 [00:04<00:03, 1223.80it/s]     59%|█████▉    | 5909/10000 [00:04<00:03, 1224.01it/s]     60%|██████    | 6032/10000 [00:04<00:03, 1225.35it/s]     62%|██████▏   | 6155/10000 [00:05<00:03, 1222.78it/s]     63%|██████▎   | 6278/10000 [00:05<00:03, 1222.95it/s]     64%|██████▍   | 6401/10000 [00:05<00:02, 1223.04it/s]     65%|██████▌   | 6525/10000 [00:05<00:02, 1225.64it/s]     66%|██████▋   | 6649/10000 [00:05<00:02, 1227.91it/s]     68%|██████▊   | 6772/10000 [00:05<00:02, 1226.15it/s]     69%|██████▉   | 6895/10000 [00:05<00:02, 1224.96it/s]     70%|███████   | 7018/10000 [00:05<00:02, 1224.62it/s]     71%|███████▏  | 7142/10000 [00:05<00:02, 1228.51it/s]     73%|███████▎  | 7265/10000 [00:05<00:02, 1223.14it/s]     74%|███████▍  | 7389/10000 [00:06<00:02, 1225.65it/s]     75%|███████▌  | 7513/10000 [00:06<00:02, 1227.76it/s]     76%|███████▋  | 7636/10000 [00:06<00:01, 1221.68it/s]     78%|███████▊  | 7759/10000 [00:06<00:01, 1223.02it/s]     79%|███████▉  | 7883/10000 [00:06<00:01, 1225.99it/s]     80%|████████  | 8006/10000 [00:06<00:01, 1226.62it/s]     81%|████████▏ | 8129/10000 [00:06<00:01, 1224.95it/s]     83%|████████▎ | 8252/10000 [00:06<00:01, 1225.67it/s]     84%|████████▍ | 8375/10000 [00:06<00:01, 1225.23it/s]     85%|████████▍ | 8498/10000 [00:06<00:01, 1222.35it/s]     86%|████████▌ | 8621/10000 [00:07<00:01, 1223.58it/s]     87%|████████▋ | 8744/10000 [00:07<00:01, 1225.43it/s]     89%|████████▊ | 8868/10000 [00:07<00:00, 1227.45it/s]     90%|████████▉ | 8991/10000 [00:07<00:00, 1227.79it/s]     91%|█████████ | 9114/10000 [00:07<00:00, 1225.95it/s]     92%|█████████▏| 9237/10000 [00:07<00:00, 1222.79it/s]     94%|█████████▎| 9360/10000 [00:07<00:00, 1222.13it/s]     95%|█████████▍| 9483/10000 [00:07<00:00, 1221.95it/s]     96%|█████████▌| 9607/10000 [00:07<00:00, 1225.38it/s]     97%|█████████▋| 9730/10000 [00:07<00:00, 1225.11it/s]     99%|█████████▊| 9853/10000 [00:08<00:00, 1225.88it/s]    100%|█████████▉| 9976/10000 [00:08<00:00, 1226.04it/s]    100%|██████████| 10000/10000 [00:08<00:00, 1223.90it/s]
    Resulting samples with prior model lower bounds of [-1,-10,-10,-10], upper bounds of [2,10,10,10]




.. GENERATED FROM PYTHON SOURCE LINES 886-888

Why do you think the posterior distribution looks like this?


.. GENERATED FROM PYTHON SOURCE LINES 891-893

--------------


.. GENERATED FROM PYTHON SOURCE LINES 896-906

Challenge: Change the data uncertainty
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To change the data uncertainty we increase ``sigma`` and then redefine
the log-Likelihood.

Here we increase the assumed data standard deviation by a factor of of
50! So we are telling the inversion that the data are far less accurate
than they actually are.


.. GENERATED FROM PYTHON SOURCE LINES 906-915

.. code-block:: Python


    sigma = 50.0                                     # common noise standard deviation
    Cdinv = np.eye(len(data_y))/(sigma**2)      # inverse data covariance matrix

    def log_likelihood(model):
        y_synthetics = forward(model)
        residual = data_y - y_synthetics
        return -0.5 * residual @ (Cdinv @ residual).T








.. GENERATED FROM PYTHON SOURCE LINES 920-922

Lets return the prior to the original bounds.


.. GENERATED FROM PYTHON SOURCE LINES 922-931

.. code-block:: Python


    m_lower_bound = np.ones(4) * (-10.)             # lower bound for uniform prior
    m_upper_bound = np.ones(4) * 10                 # upper bound for uniform prior

    def log_prior(model):    # uniform distribution
        for i in range(len(m_lower_bound)):
            if model[i] < m_lower_bound[i] or model[i] > m_upper_bound[i]: return -np.inf
        return 0.0 # model lies within bounds -> return log(1)








.. GENERATED FROM PYTHON SOURCE LINES 936-964

Your challenge is then to tell CoFI that the Likelihood and prior have
changed and then to rerun the sample, and plot results.

|Upload to Jamboard 3|

Feel free to start from the code below:

::

   ######## CoFI BaseProblem - update information
   inv_problem.set_log_likelihood(<CHANGE ME>)
   inv_problem.set_log_prior(<CHANGE ME>)

   ######## CoFI Inversion - run it
   inv_5 = Inversion(inv_problem, inv_options_3)
   inv_result_5 = inv_5.run()

   flat_samples = inv_result_5.sampler.get_chain(discard=300, thin=30, flat=True)
   inds = np.random.randint(len(flat_samples), size=100) # get a random selection from posterior ensemble

   print("Resulting samples from changed data uncertainty")
   plot_data()
   plot_models(flat_samples[inds])
   plot_model(x,true_y, "True model", color="darkorange")

.. |Upload to Jamboard 3| image:: https://img.shields.io/badge/Click%20&%20upload%20your%20results%20to-Jamboard-lightgrey?logo=jamboard&style=for-the-badge&color=fcbf49&labelColor=edede9
   :target: https://jamboard.google.com/d/1ewIkma6uTeNWu7ACEC3vG4J0FNPQZVLdlQLhyeLh-qM/edit?usp=sharing


.. GENERATED FROM PYTHON SOURCE LINES 964-969

.. code-block:: Python


    # Copy the template above, Replace <CHANGE ME> with your answer










.. GENERATED FROM PYTHON SOURCE LINES 974-976

The answer is in the next cells if you want to run them.


.. GENERATED FROM PYTHON SOURCE LINES 976-995

.. code-block:: Python


    #@title Solution

    ######## CoFI BaseProblem - update information
    inv_problem.set_log_likelihood(log_likelihood)
    inv_problem.set_log_prior(log_prior)

    ######## CoFI Inversion - run it
    inv_5 = Inversion(inv_problem, inv_options_3)
    inv_result_5 = inv_5.run()

    flat_samples = inv_result_5.sampler.get_chain(discard=300, thin=30, flat=True)
    inds = np.random.randint(len(flat_samples), size=100) # get a random selection from posterior ensemble

    print("Resulting samples from changed data uncertainty")
    plot_data()
    plot_models(flat_samples[inds])
    plot_model(x,true_y, "True model", color="darkorange")




.. image-sg:: /tutorials/generated/images/sphx_glr_linear_regression_010.png
   :alt: linear regression
   :srcset: /tutorials/generated/images/sphx_glr_linear_regression_010.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/10000 [00:00<?, ?it/s]      1%|▏         | 126/10000 [00:00<00:07, 1255.11it/s]      3%|▎         | 264/10000 [00:00<00:07, 1326.04it/s]      4%|▍         | 402/10000 [00:00<00:07, 1347.53it/s]      5%|▌         | 541/10000 [00:00<00:06, 1361.62it/s]      7%|▋         | 680/10000 [00:00<00:06, 1368.32it/s]      8%|▊         | 818/10000 [00:00<00:06, 1371.88it/s]     10%|▉         | 956/10000 [00:00<00:06, 1370.45it/s]     11%|█         | 1095/10000 [00:00<00:06, 1374.84it/s]     12%|█▏        | 1233/10000 [00:00<00:06, 1370.89it/s]     14%|█▎        | 1372/10000 [00:01<00:06, 1374.03it/s]     15%|█▌        | 1510/10000 [00:01<00:06, 1369.40it/s]     16%|█▋        | 1649/10000 [00:01<00:06, 1374.91it/s]     18%|█▊        | 1789/10000 [00:01<00:05, 1381.09it/s]     19%|█▉        | 1928/10000 [00:01<00:05, 1383.31it/s]     21%|██        | 2067/10000 [00:01<00:05, 1379.06it/s]     22%|██▏       | 2205/10000 [00:01<00:05, 1376.36it/s]     23%|██▎       | 2345/10000 [00:01<00:05, 1381.55it/s]     25%|██▍       | 2484/10000 [00:01<00:05, 1379.79it/s]     26%|██▋       | 2625/10000 [00:01<00:05, 1386.61it/s]     28%|██▊       | 2764/10000 [00:02<00:05, 1386.84it/s]     29%|██▉       | 2903/10000 [00:02<00:05, 1385.63it/s]     30%|███       | 3042/10000 [00:02<00:05, 1380.03it/s]     32%|███▏      | 3181/10000 [00:02<00:04, 1378.08it/s]     33%|███▎      | 3319/10000 [00:02<00:04, 1376.38it/s]     35%|███▍      | 3457/10000 [00:02<00:04, 1376.43it/s]     36%|███▌      | 3595/10000 [00:02<00:04, 1372.03it/s]     37%|███▋      | 3733/10000 [00:02<00:04, 1372.93it/s]     39%|███▊      | 3872/10000 [00:02<00:04, 1375.43it/s]     40%|████      | 4011/10000 [00:02<00:04, 1376.91it/s]     41%|████▏     | 4149/10000 [00:03<00:04, 1374.40it/s]     43%|████▎     | 4288/10000 [00:03<00:04, 1377.72it/s]     44%|████▍     | 4426/10000 [00:03<00:04, 1378.36it/s]     46%|████▌     | 4566/10000 [00:03<00:03, 1382.75it/s]     47%|████▋     | 4705/10000 [00:03<00:03, 1382.10it/s]     48%|████▊     | 4844/10000 [00:03<00:03, 1381.30it/s]     50%|████▉     | 4983/10000 [00:03<00:03, 1377.38it/s]     51%|█████     | 5123/10000 [00:03<00:03, 1383.14it/s]     53%|█████▎    | 5263/10000 [00:03<00:03, 1385.29it/s]     54%|█████▍    | 5402/10000 [00:03<00:03, 1380.28it/s]     55%|█████▌    | 5541/10000 [00:04<00:03, 1378.84it/s]     57%|█████▋    | 5679/10000 [00:04<00:03, 1373.87it/s]     58%|█████▊    | 5818/10000 [00:04<00:03, 1376.21it/s]     60%|█████▉    | 5956/10000 [00:04<00:02, 1370.52it/s]     61%|██████    | 6094/10000 [00:04<00:02, 1372.92it/s]     62%|██████▏   | 6233/10000 [00:04<00:02, 1375.60it/s]     64%|██████▎   | 6372/10000 [00:04<00:02, 1378.69it/s]     65%|██████▌   | 6511/10000 [00:04<00:02, 1380.55it/s]     66%|██████▋   | 6650/10000 [00:04<00:02, 1380.84it/s]     68%|██████▊   | 6789/10000 [00:04<00:02, 1381.15it/s]     69%|██████▉   | 6928/10000 [00:05<00:02, 1377.93it/s]     71%|███████   | 7067/10000 [00:05<00:02, 1380.36it/s]     72%|███████▏  | 7207/10000 [00:05<00:02, 1383.62it/s]     73%|███████▎  | 7346/10000 [00:05<00:01, 1376.59it/s]     75%|███████▍  | 7485/10000 [00:05<00:01, 1379.68it/s]     76%|███████▌  | 7623/10000 [00:05<00:01, 1374.75it/s]     78%|███████▊  | 7762/10000 [00:05<00:01, 1377.02it/s]     79%|███████▉  | 7902/10000 [00:05<00:01, 1382.58it/s]     80%|████████  | 8041/10000 [00:05<00:01, 1378.73it/s]     82%|████████▏ | 8180/10000 [00:05<00:01, 1380.52it/s]     83%|████████▎ | 8320/10000 [00:06<00:01, 1384.65it/s]     85%|████████▍ | 8459/10000 [00:06<00:01, 1384.91it/s]     86%|████████▌ | 8598/10000 [00:06<00:01, 1383.79it/s]     87%|████████▋ | 8737/10000 [00:06<00:00, 1383.42it/s]     89%|████████▉ | 8876/10000 [00:06<00:00, 1379.58it/s]     90%|█████████ | 9017/10000 [00:06<00:00, 1387.07it/s]     92%|█████████▏| 9156/10000 [00:06<00:00, 1386.05it/s]     93%|█████████▎| 9295/10000 [00:06<00:00, 1384.86it/s]     94%|█████████▍| 9434/10000 [00:06<00:00, 1383.75it/s]     96%|█████████▌| 9573/10000 [00:06<00:00, 1384.45it/s]     97%|█████████▋| 9712/10000 [00:07<00:00, 1385.36it/s]     99%|█████████▊| 9851/10000 [00:07<00:00, 1382.80it/s]    100%|█████████▉| 9990/10000 [00:07<00:00, 1382.58it/s]    100%|██████████| 10000/10000 [00:07<00:00, 1377.85it/s]
    Resulting samples from changed data uncertainty




.. GENERATED FROM PYTHON SOURCE LINES 1000-1045

Challenge: Change the number of walkers / steps in the McMC algorithm (optional)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Now lets decrease the number of steps performed by the McMC algorithm.
It will be faster but perform less exploration of the model parameters.

We suggest you reduce the number of steps taken by all 32 random walkers
and see how it affects the posterior ensemble.

|Upload to Jamboard 4|

You can start from code template below:

::

   # change number of steps
   nsteps = <CHANGE ME>              # instead of 10000

   # change number of walkers
   nwalkers = <CHANGE ME>            # instead of 32
   walkers_start = np.zeros(nparams) + 1e-4 * np.random.randn(nwalkers, ndim)

   # let's return to the old uncertainty settings
   sigma = 1.0                                     # common noise standard deviation
   Cdinv = np.eye(len(data_y))/(sigma**2)      # inverse data covariance matrix

   ######## CoFI InversionOptions - get a different tool
   inv_options_3.set_params(nsteps=nsteps, nwalkers=nwalkers, initial_state=walkers_start)

   ######## CoFI Inversion - run it
   inv_6 = Inversion(inv_problem, inv_options_3)
   inv_result_6 = inv_6.run()

   ######## CoFI InversionResult - plot result
   flat_samples = inv_result_6.sampler.get_chain(discard=300, thin=30, flat=True)
   inds = np.random.randint(len(flat_samples), size=10) # get a random selection from posterior ensemble

   print(f"Inference results from {nsteps} steps and {nwalkers} walkers")
   plot_data()
   plot_models(flat_samples[inds])
   plot_model(x,true_y, "True model", color="darkorange")

.. |Upload to Jamboard 4| image:: https://img.shields.io/badge/Click%20&%20upload%20your%20results%20to-Jamboard-lightgrey?logo=jamboard&style=for-the-badge&color=fcbf49&labelColor=edede9
   :target: https://jamboard.google.com/d/1vAm3dpaI4UTZiFXzb6vEku8AlVWUw7PRxz8KJk-dVf8/edit?usp=sharing


.. GENERATED FROM PYTHON SOURCE LINES 1045-1050

.. code-block:: Python


    # Copy the template above, Replace <CHANGE ME> with your answer










.. GENERATED FROM PYTHON SOURCE LINES 1052-1082

.. code-block:: Python


    #@title Solution

    # change number of steps
    nsteps = 400              # instead of 10000

    # change number of walkers
    nwalkers = 30             # instead of 32
    walkers_start = np.zeros(nparams) + 1e-4 * np.random.randn(nwalkers, ndim)

    # let's return to the old uncertainty settings
    sigma = 1.0                                     # common noise standard deviation
    Cdinv = np.eye(len(data_y))/(sigma**2)      # inverse data covariance matrix

    ######## CoFI InversionOptions - get a different tool
    inv_options_3.set_params(nsteps=nsteps, nwalkers=nwalkers, initial_state=walkers_start)

    ######## CoFI Inversion - run it
    inv_6 = Inversion(inv_problem, inv_options_3)
    inv_result_6 = inv_6.run()

    ######## CoFI InversionResult - plot result
    flat_samples = inv_result_6.sampler.get_chain(discard=300, thin=30, flat=True)
    inds = np.random.randint(len(flat_samples), size=10) # get a random selection from posterior ensemble

    print(f"Inference results from {nsteps} steps and {nwalkers} walkers")
    plot_data()
    plot_models(flat_samples[inds])
    plot_model(x,true_y, "True model", color="darkorange")




.. image-sg:: /tutorials/generated/images/sphx_glr_linear_regression_011.png
   :alt: linear regression
   :srcset: /tutorials/generated/images/sphx_glr_linear_regression_011.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/400 [00:00<?, ?it/s]     30%|███       | 121/400 [00:00<00:00, 1202.67it/s]     60%|██████    | 242/400 [00:00<00:00, 1204.07it/s]     91%|█████████ | 363/400 [00:00<00:00, 1204.53it/s]    100%|██████████| 400/400 [00:00<00:00, 1203.61it/s]
    Inference results from 400 steps and 30 walkers




.. GENERATED FROM PYTHON SOURCE LINES 1087-1095

--------------

Where to next?
--------------

-  Linear regression with Eustatic Sea-level data - `link to
   notebook <https://github.com/inlab-geo/cofi-examples/blob/main/examples/linear_regression/linear_regression_sealevel.ipynb>`__


.. GENERATED FROM PYTHON SOURCE LINES 1098-1103

--------------

Watermark
---------


.. GENERATED FROM PYTHON SOURCE LINES 1103-1109

.. code-block:: Python


    watermark_list = ["cofi", "numpy", "scipy", "matplotlib", "emcee", "arviz"]
    for pkg in watermark_list:
        pkg_var = __import__(pkg)
        print(pkg, getattr(pkg_var, "__version__"))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    cofi 0.2.7
    numpy 1.24.4
    scipy 1.12.0
    matplotlib 3.8.3
    emcee 3.1.4
    arviz 0.17.0




.. GENERATED FROM PYTHON SOURCE LINES 1110-1110

sphinx_gallery_thumbnail_number = -1


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 29.038 seconds)


.. _sphx_glr_download_tutorials_generated_linear_regression.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: linear_regression.ipynb <linear_regression.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: linear_regression.py <linear_regression.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
