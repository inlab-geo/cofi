{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Polynomial Linear Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In\nColab](https://img.shields.io/badge/open%20in-Colab-b5e2fa?logo=googlecolab&style=flat-square&color=ffd670)](https://colab.research.google.com/github/inlab-geo/cofi-examples/blob/main/examples/linear_regression/linear_regression.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you are running this notebook locally, make sure you've followed\n[steps\nhere](https://github.com/inlab-geo/cofi-examples#run-the-examples-with-cofi-locally)\nto set up the environment. (This\n[environment.yml](https://github.com/inlab-geo/cofi-examples/blob/main/envs/environment.yml)\nfile specifies a list of packages required to run the notebooks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This tutorial focusses on regression - that is, fitting curves to\ndatasets. We will look at a simple linear regression example with\n`cofi`.\n\nTo begin with, we will work with polynomial curves,\n\n$$y(x) = \\sum_{n=0}^N m_n x^n\\,.$$\n\nHere, $N$ is the 'order' of the polynomial: if N=1 we have a straight\nline, if N=2 it will be a quadratic, and so on. The $m_n$ are the 'model\ncoefficients'.\n\nWe have a set of noisy data values, Y, measured at known locations, X,\nand wish to find the best fit degree 3 polynomial.\n\nThe function we are going to fit is: $y=-6-5x+2x^2+x^3$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction\n\nIn the workflow of `cofi`, there are three main components:\n`BaseProblem`, `InversionOptions`, and `Inversion`.\n\n-   `BaseProblem` defines three things: 1) the forward problem; 2) model\n    parameter space (the unknowns); and 3) other information about the\n    inverse problem we are solving, such as the jacobian matrix\n    (i.e.\u00a0design matrix for our linear problem) for the least squares\n    solver we will be using initially in the following\n-   `InversionOptions` describes details about how one wants to run the\n    inversion, including the inversion approach, backend tool and\n    solver-specific parameters.\n-   `Inversion` can be seen as an inversion engine that takes in the\n    above two as information, and will produce an `InversionResult` upon\n    running.\n\nFor each of the above components, there's a `summary()` method to check\nthe current status.\n\nSo a common workflow includes 4 steps:\n\n1.  we begin by defining the `BaseProblem`. This can be done through a\n    series of set functions\n    `python     inv_problem = BaseProblem()     inv_problem.set_objective(some_function_here)     inv_problem.set_initial_model(a_starting_point)`\n\n2.  define `InversionOptions`. Some useful methods include:\n\n    -   `set_solving_method()` and `suggest_tools()`. Once you've set a\n        solving method (from \"least squares\" and \"optimization\", more\n        will be supported), you can use `suggest_tools()` to see a list\n        of backend tools to choose from.\n\n3.  start an `Inversion`. This step is common:\n\n        inv = Inversion(inv_problem, inv_options)\n        result = inv.run()\n\n4.  analyse the result, workflow and redo your experiments with\n    different `InversionOptions` objects\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n\n# 1. Import modules\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------- #\n#                                                          #\n#     Uncomment below to set up environment on \"colab\"     #\n#                                                          #\n# -------------------------------------------------------- #\n\n# !pip install -U cofi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nimport arviz as az\n\nfrom cofi import BaseProblem, InversionOptions, Inversion\nfrom cofi.utils import QuadraticReg\n\nnp.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n\n# 2. Define the problem\n\nHere we compute $y(x)$ for multiple $x$-values simultaneously, so write\nthe forward operator in the following form:\n\n$$\\begin{aligned}\n\\left(\\begin{array}{c}y_1\\\\y_2\\\\\\vdots\\\\y_N\\end{array}\\right) = \\left(\\begin{array}{ccc}1&x_1&x_1^2&x_1^3\\\\1&x_2&x_2^2&x_2^3\\\\\\vdots&\\vdots&\\vdots\\\\1&x_N&x_N^2&x_N^3\\end{array}\\right)\\left(\\begin{array}{c}m_0\\\\m_1\\\\m_2\\end{array}\\right)\n\\end{aligned}$$\n\nThis clearly has the required general form, $\\mathbf{y=Gm}$, and so the\nbest-fitting model can be identified using the least-squares algorithm.\n\nIn the following code block, we'll define the forward function and\ngenerate some random data points as our dataset.\n\n$$\\begin{aligned}\n\\begin{align}\n\\text{forward}(\\textbf{m}) &= \\textbf{G}\\textbf{m}\\\\\n&= \\text{basis_func}(\\textbf{x})\\cdot\\textbf{m}\n\\end{align}\n\\end{aligned}$$\n\nwhere:\n\n-   $\\text{forward}$ is the forward function that takes in a model and\n    produces synthetic data,\n\n-   $\\textbf{m}$ is the model vector,\n\n-   $\\textbf{G}$ is the basis matrix (i.e.\u00a0design matrix) of this linear\n    regression problem and looks like the following:\n\n    $$\\begin{aligned}\n    \\left(\\begin{array}{ccc}1&x_1&x_1^2&x_1^3\\\\1&x_2&x_2^2&x_2^3\\\\\\vdots&\\vdots&\\vdots\\\\1&x_N&x_N^2&x_N^3\\end{array}\\right)\n    \\end{aligned}$$\n\n-   $\\text{basis_func}$ is the basis function that converts $\\textbf{x}$\n    into $\\textbf{G}$\n\nRecall that the function we are going to fit is: $y=-6-5x+2x^2+x^3$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# generate data with random Gaussian noise\ndef basis_func(x):\n    return np.array([x**i for i in range(4)]).T                           # x -> G\n_m_true = np.array([-6,-5,2,1])                                           # m\nsample_size = 20                                                          # N\nx = np.random.choice(np.linspace(-3.5,2.5), size=sample_size)             # x\ndef forward_func(m):\n    return basis_func(x) @ m                                              # m -> y_synthetic\ny_observed = forward_func(_m_true) + np.random.normal(0,1,sample_size)    # d\n\n############## PLOTTING ###############################################################\n_x_plot = np.linspace(-3.5,2.5)\n_G_plot = basis_func(_x_plot)\n_y_plot = _G_plot @ _m_true\nplt.figure(figsize=(12,8))\nplt.plot(_x_plot, _y_plot, color=\"darkorange\", label=\"true model\")\nplt.scatter(x, y_observed, color=\"lightcoral\", label=\"observed data\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we define the problem in `cofi` - in other words,we set the problem\ninformation for a `BaseProblem` object.\n\nFrom [this\npage](https://cofi.readthedocs.io/en/latest/api/generated/cofi.BaseProblem.html#set-methods)\nyou'll see a list of functions/properties that can be set to\n`BaseProblem`.\n\nOther helper methods for `BaseProblem` include:\n\n-   `defined_components()` (review what have been set)\n-   `summary()` (better displayed information)\n-   `suggest_tools()`\n\nWe refer readers to [cofi's API reference\npage](https://cofi.readthedocs.io/en/latest/api/generated/cofi.BaseProblem.html)\nfor details about all these methods.\n\nSince we are dealing with a linear problem, the design matrix\n$\\textbf{G}$ is the Jacobian of the forward function with respect to the\nmodel. This information will be useful when the inversion solver is a\nlinear system solver (as we'll demonstrate firstly in the next section).\n\nFor a linear system solver, only the data observations vector and the\nJacobian matrix are needed. We thus set them to our `BaseProblem`\nobject.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# define the problem in cofi\ninv_problem = BaseProblem()\ninv_problem.name = \"Polynomial Regression\"\ninv_problem.set_data(y_observed)\ninv_problem.set_jacobian(basis_func(x))\n\ninv_problem.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n\n# 3. Define the inversion options\n\nAs mentioned above, an `InversionOptions` object contains everything\nyou'd like to define regarding how the inversion is to be run.\n\nFrom [this\npage](https://cofi.readthedocs.io/en/latest/api/generated/cofi.InversionOptions.html)\nyou'll see the methods for `InversionOptions`.\n\nIn general: 1. we use `InversionOptions.set_tool(\"tool_name\")` to set\nwhich backend tool you'd like to use 2. then with\n`InversionOptions.set_params(p1=val1, p2=val2, ...)` you can set\nsolver-specific parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inv_options = InversionOptions()\ninv_options.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have a **suggesting system** that is being improved at the moment, so\nthat you can see what backend tools are available based on the\ncategories of inversion approaches you'd like to use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inv_options.suggest_tools()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Having seen what a default `InversionOptions` object look like, we\ncustomise the inversion process by constraining the solving approach:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inv_options.set_solving_method(\"matrix solvers\")\ninv_options.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n\nAs the \"summary\" suggested, you've set the solving method, so you can\nskip the step of setting a backend tool because there's a default one.\n\nIf there are more than one backend tool options, then the following\nfunction shows available options and set your desired backend solver.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inv_options.suggest_tools()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also set the backend tool directly (as following), without the\ncall to `inv_options.set_solving_method()` above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inv_options.set_tool(\"scipy.linalg.lstsq\")\ninv_options.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n\n# 4. Start an inversion\n\nThis step is common for most cases. We've specified our problem as a\n`BaseProblem` object, and we've defined how to run the inversion as an\n`InversionOptions` object.\n\nTaking them both in, an `Inversion` object knows all the information and\nis an engine to actually perform the inversion.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inv = Inversion(inv_problem, inv_options)\ninv.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's run it!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inv_result = inv.run()\ninv_result.success"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The inversion result returned by `inv.run()` is an instance of\n`InversionResult`.\n\nSee [this documentation\npage](https://cofi.readthedocs.io/en/latest/api/generated/cofi.InversionResult.html)\nfor details about what can be done with the resulting object.\n\nResults returned by different backend tools will have different extra\ninformation. But there are two common things - they all have a `success`\nstatus (as a boolean) and a `model`/`sampler` result.\n\nSimilar to the other class objects, you can see what's inside it with\nthe `summary()` method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inv_result.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n\n# 5. Check back your problem setting, inversion setting & result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A summary view of the `Inversion` object shows information about the\nwhole inversion process, including how the problem is defined, how the\ninversion is defined to be run, as well as what the results are (if\nany).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inv.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's plot the predicted curve and compare it to the data and\nground truth.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y_synthetic = forward_func(inv_result.model)\n\n############## PLOTTING ###############################################################\n_x_plot = np.linspace(-3.5,2.5)\n_G_plot = basis_func(_x_plot)\n_y_plot = _G_plot @ _m_true\n_y_synth = _G_plot @ inv_result.model\nplt.figure(figsize=(12,8))\nplt.plot(_x_plot, _y_plot, color=\"darkorange\", label=\"true model\")\nplt.plot(_x_plot, _y_synth, color=\"seagreen\", label=\"least squares solution\")\nplt.scatter(x, y_observed, color=\"lightcoral\", label=\"original data\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we see the least squares solver (green curve) fits all of the data\nwell and is a close approximation of the true curve (orange).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n\n# 6. Summary: a cleaner version of the above example\n\nFor review purpose, here are the minimal set of commands we've used to\nproduce the above result:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "######## Import and set random seed\nimport numpy as np\nfrom cofi import BaseProblem, InversionOptions, Inversion\n\nnp.random.seed(42)\n\n######## Write code for your forward problem\n_m_true = np.array([-6,-5,2,1])                                            # m\n_sample_size = 20                                                          # N\nx = np.random.choice(np.linspace(-3.5,2.5), size=_sample_size)             # x\ndef basis_func(x):\n    return np.array([x**i for i in range(4)]).T                            # x -> G\ndef forward_func(m): \n    return (np.array([x**i for i in range(4)]).T) @ m                      # m -> y_synthetic\ny_observed = forward_func(_m_true) + np.random.normal(0,1,_sample_size)    # d\n\n######## Attach above information to a `BaseProblem`\ninv_problem = BaseProblem()\ninv_problem.name = \"Polynomial Regression\"\ninv_problem.set_data(y_observed)\ninv_problem.set_jacobian(basis_func(x))\n\n######## Specify how you'd like the inversion to run (via an `InversionOptions`)\ninv_options = InversionOptions()\ninv_options.set_tool(\"scipy.linalg.lstsq\")\n\n######## Pass `BaseProblem` and `InversionOptions` into `Inversion` and run\ninv = Inversion(inv_problem, inv_options)\ninv_result = inv.run()\n\n######## Now check out the result\nprint(f\"The inversion result from `scipy.linalg.lstsq`: {inv_result.model}\\n\")\ninv_result.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n\n# 7. Switching to a different inversion approach\n\nWe've seen how this linear regression problem is solved with a linear\nsystem solver. It's time to see `cofi`'s capability to switch between\ndifferent inversion approaches easily.\n\n## 7.1. optimization\n\nAny linear problem $\\textbf{y} = \\textbf{G}\\textbf{m}$ can also be\nsolved by minimizing the squares of the residual of the linear\nequations, e.g.\u00a0$\\textbf{r}^T \\textbf{r}$ where\n$\\textbf{r}=\\textbf{y}-\\textbf{G}\\textbf{m}$.\n\nSo we first use a plain optimizer `scipy.optimize.minimize` to\ndemonstrate this ability.\n\nFor this backend solver to run successfully, some additional information\nshould be provided, otherwise you'll see an error to notify what\nadditional information is required by the solver.\n\nThere are several ways to provide the information needed to solve an\ninverse problem with CoFI. In the example below we provide functions to\ncalculate the data and the optional regularization. CoFI then generates\nthe objective function for us based on the information provided. The\nalternative to this would be to directly provide objective function to\nCoFI.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "######## Provide additional information\ninv_problem.set_initial_model(np.ones(4))\ninv_problem.set_forward(forward_func)\ninv_problem.set_data_misfit(\"least squares\")\ninv_problem.set_regularization(0.02 * QuadraticReg(model_shape=(4,)))      # optional\n\n######## Set a different tool\ninv_options_2 = InversionOptions()\ninv_options_2.set_tool(\"scipy.optimize.minimize\")\n\n######## Run it\ninv_2 = Inversion(inv_problem, inv_options_2)\ninv_result_2 = inv_2.run()\n\n######## Check result\nprint(f\"The inversion result from `scipy.optimize.minimize`: {inv_result_2.model}\\n\")\ninv_result_2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "######## Plot all together\n_x_plot = np.linspace(-3.5,2.5)\n_G_plot = basis_func(_x_plot)\n_y_plot = _G_plot @ _m_true\n_y_synth = _G_plot @ inv_result.model\n_y_synth_2 = _G_plot @ inv_result_2.model\nplt.figure(figsize=(12,8))\nplt.plot(_x_plot, _y_plot, color=\"darkorange\", label=\"true model\")\nplt.plot(_x_plot, _y_synth, color=\"seagreen\", label=\"least squares solution\")\nplt.plot(_x_plot, _y_synth_2, color=\"cornflowerblue\", label=\"optimization solution\")\nplt.scatter(x, y_observed, color=\"lightcoral\", label=\"original data\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we see the (blue curve) is also a relatively good approximation of\nthe true curve (orange).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7.2. Sampling\n\nWe've seen the same regression problem solved with a linear system\nsolver and an optimizer - how about sampling?\n\n## Background (if you're relatively new to this)\n\nBefore we show you an example of how to solve this problem from a\nBayesian sampling perspective, let's switch to a slightly different\nmindset:\n\n1.  Instead of getting a result as a **single \"best-fit\"** model, it's\n    worthwhile to obtain an **ensemble** of models\n2.  How to *express* such an ensemble of models? It's uncertain where\n    the true model is, but given a) the data and b) some prior knowledge\n    about the model, we can express it as a **probability\n    distribution**, where $p(\\text{model})$ is the probability at which\n    the $\\text{model}$ is true.\n3.  How to *estimate* this distribution then? There are various ways,\n    and **sampling** is a typical one of them.\n\nIn a sampling approach, there are typically multiple walkers that start\nfrom some initial points (initial guesses of the models) and take steps\nin the model space (the set of all possible models). With a Markov chain\nMonte Carlo (McMC) sampler, the walkers move step by step, and determine\nwhether to keep the new sample based on evaluation of the posterior\nprobability density we provide, with some randomness.\n\nThe sampler seeks to recover the unknown **posterior distribution** as\nefficiently as possible and different samplers employ different\nstrategies to determine a step (i.e.\u00a0perturbation to the current model)\nthat finds a balance between the exploration and exploitation.\n\nStarting from the **Bayes theorem**:\n\n$$p(A|B) = \\frac{p(B|A)p(A)}{p(B)}$$\n\nThe unknowns are model parameters, so we set $A$ to be $\\textbf{m}$\n(model), and $B$ to be $\\textbf{d}$ (data). Since the marginal\ndistribution $p(\\textbf{d})$ is assumed to be unrelated to the\n$\\textbf{m}$, we get the following relationship:\n\n$$p(\\textbf{m}|\\textbf{d}) \\propto p(\\textbf{d}|\\textbf{m}) p(\\textbf{m})$$\n\nwhere:\n\n-   $p(\\textbf{m}|\\textbf{d})$ (posterior) is the probability of a model\n    given data observations\n-   $p(\\textbf{d}|\\textbf{m})$ (likelihood) is the probability of which\n    data is observed given a certain model\n-   $p(\\textbf{m})$ (prior) is the probability of a certain model and\n    reflects your belief / domain knowledge on the model\n\n## Coding\n\nMost sampler tools require the logarithm of the probability.\n\n$$\\log(\\text{posterior}) = \\log(\\text{likelihood}) + \\log(\\text{prior})$$\n\nSo in `cofi`, you can either define:\n\n-   log of the posterior, using `BaseProblem.set_log_posterior`\n    ([ref](https://cofi.readthedocs.io/en/latest/api/generated/cofi.BaseProblem.html#cofi.BaseProblem.set_log_posterior)),\n    or\n-   log of prior and log of likelihood, using\n    `BaseProblem.set_log_prior()`\n    ([ref](https://cofi.readthedocs.io/en/latest/api/generated/cofi.BaseProblem.html#cofi.BaseProblem.set_log_prior))\n    and `BaseProblem.set_log_likelihood()`\n    ([ref](https://cofi.readthedocs.io/en/latest/api/generated/cofi.BaseProblem.html#cofi.BaseProblem.set_log_likelihood))\n\nWe use the second option in this demo.\n\n### Likelihood\n\nTo measure the probability of the observed y values given those\npredicted by our polynomial curve we specify a Likelihood function\n$p({\\mathbf d}_{obs}| {\\mathbf m})$\n\n$$p({\\mathbf d}_{obs} | {\\mathbf m}) \\propto \\exp \\left\\{- \\frac{1}{2} ({\\mathbf d}_{obs}-{\\mathbf d}_{pred}({\\mathbf m}))^T C_D^{-1} ({\\mathbf d}_{obs}-{\\mathbf d}_{pred}({\\mathbf m})) \\right\\}$$\n\nwhere ${\\mathbf d}_{obs}$ represents the observed y values and\n${\\mathbf d}_{pred}({\\mathbf m})$ are those predicted by the polynomial\nmodel $({\\mathbf m})$. The Likelihood is defined as the probability of\nobserving the data actually observed, given an model. For sampling we\nwill only need to evaluate the log of the Likelihood,\n$\\log p({\\mathbf d}_{obs} | {\\mathbf m})$. To do so, we require the\ninverse data covariance matrix describing the statistics of the noise in\nthe data, $C_D^{-1}$ . For this problem the data errors are independent\nwith identical standard deviation in noise for each datum. Hence\n$C_D^{-1} = \\frac{1}{\\sigma^2}I$ where $\\sigma=1$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sigma = 1.0                                     # common noise standard deviation\nCdinv = np.eye(len(y_observed))/(sigma**2)      # inverse data covariance matrix\n\ndef log_likelihood(model):\n    y_synthetics = forward_func(model)\n    residual = y_observed - y_synthetics\n    return -0.5 * residual @ (Cdinv @ residual).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prior\n\nBayesian sampling requires a prior probability density function. A\ncommon problem with polynomial coefficients as model parameters is that\nit is not at all obvious what a prior should be. There are two common\nchoices.\n\nThe first is to make the prior uniform with specified bounds\n\n$$\\begin{aligned}\n\\begin{align}\np({\\mathbf m}) &= \\frac{1}{V},\\quad  l_i \\le m_i \\le u_i, \\quad (i=1,\\dots,M)\\\\\n\\\\\n         &= 0, \\quad {\\rm otherwise},\n\\end{align}\n\\end{aligned}$$\n\nwhere $l_i$ and $u_i$ are lower and upper bounds on the $i$th model\ncoefficient.\n\nThe second choice is to make the prior an unbounded Gaussian\n\n$$p({\\mathbf m}) \\propto \\exp \\left\\{- \\frac{1}{2}({\\mathbf m}-{\\mathbf m}_o)^T C_M^{-1}({\\mathbf m}-{\\mathbf m}_o)\n\\right\\},$$\n\nwhere ${\\mathbf m}_o)$ is some reference set of model coefficients, and\n$C_M^{-1}$ is an inverse model covariance describing prior information\nfor each model parameter.\n\nHere we choose a Uniform prior with\n${\\mathbf l}^T = (-10.,-10.,-10.,-10.)$, and\n${\\mathbf u}^T = (10.,10.,10.,10.)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "m_lower_bound = np.ones(4) * (-10.)             # lower bound for uniform prior\nm_upper_bound = np.ones(4) * 10                 # upper bound for uniform prior\n\ndef log_prior(model):    # uniform distribution\n    for i in range(len(m_lower_bound)):\n        if model[i] < m_lower_bound[i] or model[i] > m_upper_bound[i]: return -np.inf\n    return 0.0 # model lies within bounds -> return log(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Walkers' starting points\n\nNow we define some hyperparameters (e.g.\u00a0the number of walkers and\nsteps), and initialise the starting positions of walkers. We start all\nwalkers in a small ball about a chosen point $(0, 0, 0, 0)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nwalkers = 32\nndim = 4\nnsteps = 5000\nwalkers_start = np.array([0.,0.,0.,0.]) + 1e-4 * np.random.randn(nwalkers, ndim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we attach all above information to our `BaseProblem` and\n`InversionOptions` objects.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "######## Provide additional information\ninv_problem.set_log_prior(log_prior)\ninv_problem.set_log_likelihood(log_likelihood)\ninv_problem.set_model_shape(ndim)\n\n######## Set a different tool\ninv_options_3 = InversionOptions()\ninv_options_3.set_tool(\"emcee\")\ninv_options_3.set_params(nwalkers=nwalkers, nsteps=nsteps, initial_state=walkers_start)\n\n######## Run it\ninv_3 = Inversion(inv_problem, inv_options_3)\ninv_result_3 = inv_3.run()\n\n######## Check result\nprint(f\"The inversion result from `emcee`:\")\ninv_result_3.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analyse sampling results\n\nSampler is complete. We do not know if there have been enough walkers or\nenough samplers but we'll have a look at these results, using some\nstandard approaches.\n\nAs you've seen above, `inv_result_3` has a `sampler` attribute attached\nto it, and this contains all the information from backend sampler,\nincluding the chains on each walker, their associated posterior value,\netc. You get to access all the raw data directly by exploring this\n`inv_result_3.sampler` object.\n\nAdditionally, we can convert a sampler object into an instance of\n`arviz.InferenceData`\n([ref](https://python.arviz.org/en/latest/api/generated/arviz.InferenceData.html#arviz.InferenceData)),\nso that all the plotting functions from\n[arviz](https://python.arviz.org/en/latest/index.html) are exposed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sampler = inv_result_3.sampler\naz_idata = inv_result_3.to_arviz()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sampling performance\n\nLet's take a look at what the sampler has done. A good first step is to\nlook at the time series of the parameters in the chain. The samples can\nbe accessed using the `EnsembleSampler.get_chain()` method. This will\nreturn an array with the shape (5000, 32, 3) giving the parameter values\nfor each walker at each step in the chain. The figure below shows the\npositions of each walker as a function of the number of steps in the\nchain:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "labels = [\"m0\", \"m1\", \"m2\",\"m3\"]\naz.plot_trace(az_idata);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Autocorrelation analysis\n\nAs mentioned above, the walkers start in small distributions around some\nchosen values and then they quickly wander and start exploring the full\nposterior distribution. In fact, after a relatively small number of\nsteps, the samples seem pretty well \"burnt-in\". That is a hard statement\nto make quantitatively, but we can look at an estimate of the integrated\nautocorrelation time (see Emcee's package the -[Autocorrelation analysis\n& convergence\ntutorial](https://emcee.readthedocs.io/en/stable/tutorials/autocorr/)\nfor more details):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tau = sampler.get_autocorr_time()\nprint(f\"autocorrelation time: {tau}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Corner plot\n\nThe above suggests that only about 70 steps are needed for the chain to\n\"forget\" where it started. It's not unreasonable to throw away a few\ntimes this number of steps as \"burn-in\".\n\nLet's discard the initial 300 steps, and thin by about half the\nautocorrelation time (30 steps).\n\nLet's make one of the most useful plots you can make with your MCMC\nresults: a corner plot.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "_, axes = plt.subplots(4, 4, figsize=(14,10))\naz.plot_pair(\n    az_idata.sel(draw=slice(300,None)), \n    marginals=True, \n    reference_values=dict(zip([f\"var_{i}\" for i in range(4)], _m_true.tolist())),\n    ax = axes\n);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The corner plot shows all the one and two dimensional projections of the\nposterior probability distributions of your parameters. This is useful\nbecause it quickly demonstrates all of the covariances between\nparameters. Also, the way that you find the marginalized distribution\nfor a parameter or set of parameters using the results of the MCMC chain\nis to project the samples into that plane and then make an N-dimensional\nhistogram. That means that the corner plot shows the marginalized\ndistribution for each parameter independently in the histograms along\nthe diagonal and then the marginalized two dimensional distributions in\nthe other panels.\n\n# Predicted curves\n\nNow lets plot the a sub-sample of 100 the predicted curves from this\nposterior ensemble and compare to the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "flat_samples = sampler.get_chain(discard=300, thin=30, flat=True)\ninds = np.random.randint(len(flat_samples), size=100) # get a random selection from posterior ensemble\n_x_plot = np.linspace(-3.5,2.5)\n_G_plot = basis_func(_x_plot)\n_y_plot = _G_plot @ _m_true\nplt.figure(figsize=(12,8))\nsample = flat_samples[0]\n_y_synth = _G_plot @ sample\nplt.plot(_x_plot, _y_synth, color=\"seagreen\", label=\"Posterior samples\",alpha=0.1)\nfor ind in inds:\n    sample = flat_samples[ind]\n    _y_synth = _G_plot @ sample\n    plt.plot(_x_plot, _y_synth, color=\"seagreen\", alpha=0.1)\nplt.plot(_x_plot, _y_plot, color=\"darkorange\", label=\"true model\")\nplt.scatter(x, y_observed, color=\"lightcoral\", label=\"observed data\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Uncertainty estimates\n\nWe can now calculate some formal uncertainties based on the 16th, 50th,\nand 84th percentiles of the samples in the marginalized distributions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "solmed = np.zeros(4)\nfor i in range(ndim):\n    mcmc = np.percentile(flat_samples[:, i], [16, 50, 84])\n    solmed[i] = mcmc[1]\n    q = np.diff(mcmc)\n    # txt = \"\\mathrm{{{3}}} = {0:.3f}_{{-{1:.3f}}}^{{{2:.3f}}} \"\n    # txt = txt.format(mcmc[1], q[0], q[1], labels[i])\n    # display(Math(txt))\n    print(f\"{labels[i]} = {round(mcmc[1],3)}, (-{round(q[0],3)}, +{round(q[1],3)})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first number here is the median value of each model coefficient in\nthe posterior ensemble, while the upper and lower numbers correspond to\nthe differences between the median and the 16th and 84th percentile.\nRecall here that the true values were $m_0 = -6, m_1 = -5, m_2= 2,$ and\n$m_3 = 1$. So all are close to the median and lie within the credible\nintervals.\n\nWe can also calculate the posterior model covariance matrix and compare\nto that estimated by least squares.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "CMpost = np.cov(flat_samples.T)\nCM_std= np.std(flat_samples,axis=0)\nprint('Posterior model covariance matrix\\n',CMpost)\nprint('\\n Posterior estimate of model standard deviations in each parameter')\nfor i in range(ndim):\n    print(\"    {} {:7.4f}\".format(labels[i],CM_std[i]))\n    \ninv_problem.set_data_covariance_inv(Cdinv)\nCMlstsq = inv_problem.model_covariance(None)\nprint('\\nModel covariance matrix estimated by least squares\\n', CMlstsq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n Solution and 95% credible intervals \")\nfor i in range(ndim):\n    mcmc = np.percentile(flat_samples[:, i], [5, 50, 95])\n    print(\" {} {:7.3f} [{:7.3f}, {:7.3f}]\".format(labels[i],mcmc[1],mcmc[0],mcmc[2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n\n# Watermark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "watermark_list = [\"cofi\", \"numpy\", \"scipy\", \"matplotlib\", \"emcee\", \"arviz\"]\nfor pkg in watermark_list:\n    pkg_var = __import__(pkg)\n    print(pkg, getattr(pkg_var, \"__version__\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "sphinx_gallery_thumbnail_number = -1\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}