{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linear regression with Eustatic Sea-level data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In\nColab](https://img.shields.io/badge/open%20in-Colab-b5e2fa?logo=googlecolab&style=flat-square&color=ffd670)](https://colab.research.google.com/github/inlab-geo/cofi-examples/blob/main/examples/linear_regression/linear_regression_sealevel.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you are running this notebook locally, make sure you've followed\n[steps\nhere](https://github.com/inlab-geo/cofi-examples#run-the-examples-with-cofi-locally)\nto set up the environment. (This\n[environment.yml](https://github.com/inlab-geo/cofi-examples/blob/main/envs/environment.yml)\nfile specifies a list of packages required to run the notebooks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n\n# What we do in this notebook\n\nHere we demonstrate use of CoFI on a real dataset **linear regression**\nproblem, where we fit a polynomial function to Eustatic Sea-level\nheights.\n\n-   by solution of a linear system of equations,\n-   by optimization of a data misfit function\n-   by Bayesian sampling of a Likelihood multiplied by a prior.\n\n------------------------------------------------------------------------\n\nData set is from \"Sea level and global ice volumes from the Last Glacial\nMaximum to the Holocene\" K. Lambeck, H. Rouby, A. Purcell, Y. Sun, and\nM. Sambridge, 2014. Proc. Nat. Acad. Sci., 111, no. 43, 15296-15303,\n<doi:10.1073/pnas.1411762111>.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Environment setup (uncomment code below)\n\n# !pip install -U cofi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remember to uncomment and run the code cell below as well, as we are\ngoing to load some data from GitHub.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/inlab-geo/cofi-examples.git\n# %cd cofi-examples/examples/linear_regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linear regression\n\nLets start with some (x,y) data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#\ndef load_data_xy(filename):\n\n    f = open(filename, 'r')\n    header = f.readline()\n    lines = f.readlines()\n\n    x = np.array([])\n    y = np.array([])\n    sx = np.array([])\n    sy = np.array([])\n    for line in lines:\n        columns = line.split()\n        x = np.append(x,float(columns[0]))\n        y = np.append(y,float(columns[1]))\n        sx = np.append(sx,float(columns[2])/2.0)\n        sy = np.append(sy,float(columns[3])/2.0)\n\n    d = x,y, sy                                   # Combine into a single data structure\n\n    return d\n\ndef load_data_ref(filename):\n\n    f = open(filename, 'r')\n    lines = f.readlines()\n    dx = np.array([]) # Age data\n    dy = np.array([]) # ESL height\n    dz = np.array([]) # derivative of ESL w.r.t. age\n    for line in lines:\n        columns = line.split()\n        dx = np.append(dx,float(columns[0]))\n        dy = np.append(dy,float(columns[1]))\n    datavals = np.column_stack((dx,dy))  # Stack data\n\n    return datavals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_x,data_y,sy = load_data_xy(\"ESL-ff11-sorted.txt\")  # Load x,sx,y,sy ESL data (x=time, Y=ESL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "maxtime = 20.\nndata = np.where(data_x>maxtime)[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_x,data_y,sy = data_x[:ndata],data_y[:ndata],sy[:ndata]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now lets plot the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_data(x=data_x,y=data_y,sigma=sy,title=None):\n    fig, axes = plt.subplots(figsize=(9,6))\n    plt.errorbar(x, y, yerr=sy, fmt='.',color=\"lightcoral\",ecolor='lightgrey',ms=2)\n    plt.xlabel(' Time before present (ka)')\n    plt.ylabel(' ESL height (m)')\n    if(title != None): plt.title(title)\nplot_data(title='Eustatic sea-level')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Problem description\n\nTo begin with, we will work with polynomial curves,\n\n$$y(x) = \\sum_{j=0}^M m_j x^j\\,.$$\n\nHere, $M$ is the 'order' of the polynomial: if $M=1$ we have a straight\nline with 2 parameters, if $M=2$ it will be a quadratic with 3\nparameters, and so on. The $m_j, (j=0,\\dots M)$ are the 'model\ncoefficients' that we seek to constrain from the data.\n\nFor this class of problem the forward operator takes the following form:\n\n$$\\begin{aligned}\n\\left(\\begin{array}{c}y_0\\\\y_1\\\\\\vdots\\\\y_N\\end{array}\\right) = \\left(\\begin{array}{ccc}1&x_0&x_0^2&x_0^3\\\\1&x_1&x_1^2&x_1^3\\\\\\vdots&\\vdots&\\vdots\\\\1&x_N&x_N^2&x_N^3\\end{array}\\right)\\left(\\begin{array}{c}m_0\\\\m_1\\\\m_2\\end{array}\\right)\n\\end{aligned}$$\n\nThis clearly has the required general form, $\\mathbf{d} =G{\\mathbf m}$.\n\nwhere:\n\n-   $\\textbf{d}$ is the vector of data values, ($y_0,y_1,\\dots,y_N$);\n-   $\\textbf{m}$ is the vector of model parameters, ($m_0,m_1,m_2$);\n-   $G$ is the basis matrix (or design matrix) of this linear regression\n    problem (also called the **Jacobian** matrix for this linear\n    problem).\n\nWe have a set of noisy data values, $y_i (i=0,\\dots,N)$, measured at\nknown locations, $x_i (i=0,\\dots,N)$, and wish to find the best fit\ndegree 3 polynomial.\n\nThe function that generated our data is assumed to have independent\nGaussian random noise, ${\\cal N}(0,\\Sigma)$, with\n$(\\Sigma)_{ij} = \\delta_{ij}/\\sigma_i^2$, where the variance of the\nnoise on each datum, $\\sigma_i^2 (i=1,\\dots,N)$, differs between\nobservations and is given.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now build the Jacobian/G matrix for this problem and define a forward\nfunction which simply multiplies $\\mathbf m$ by $G$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nparams = 5 # Number of model parameters to be solved for\n\ndef jacobian(x=data_x, n=nparams):\n    return np.array([x**i for i in range(n)]).T\n\ndef forward(model):\n    return jacobian().dot(model)\n\ndef Cd_inv(sigma=sy):\n    factor= 10                                   # factor to inflate observational errors\n    return np.diag(1./sy*1./sy)/(factor**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a reference model for later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Reference model for plotting\nESLref = load_data_ref(\"ESL-f11_yonly.txt\") # Load x, y, z reference model and estimated derivative (x=time, Y=ESL, z=dESL/dt) \nndata2 = np.where(ESLref.T[0]>maxtime)[0][0]\nESLref = ESLref[:ndata2]\nref_x,ref_y = ESLref.T[0],ESLref.T[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets plot the data with the reference curve\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Some plotting utilities\ndef plot_model(x,y, label, color=None,lw=0.5):\n    plt.plot(x, y, color=color or \"green\", label=label,lw=lw)\n    #plt.xlabel(\"X\")\n    #plt.ylabel(\"ESL\")\n    plt.legend()\n\ndef plot_models(models, label=\"Posterior samples\", color=\"seagreen\", alpha=0.1,lw=0.5):\n    G = jacobian(data_x)\n    plt.plot(data_x, G.dot(models[0]), color=color, label=label, alpha=alpha,lw=lw)\n    for m in models:\n        plt.plot(data_x, G.dot(m), color=color, alpha=alpha,lw=lw)\n    plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_data(title=\"Eustatic sea-level\")\nplot_model(ref_x,ref_y, \"Reference model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we have the data and the forward model we can start to try and\nestimate the coefficients of the polynomial from the data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The structure of CoFI\n\nIn the workflow of `cofi`, there are three main components:\n`BaseProblem`, `InversionOptions`, and `Inversion`.\n\n-   `BaseProblem` defines the inverse problem including any user\n    supplied quantities such as data vector, number of model parameters\n    and measure of fit between model predictions and data.\n    `python     inv_problem = BaseProblem()     inv_problem.set_objective(some_function_here)     inv_problem.set_jacobian(some_function_here)     inv_problem.set_initial_model(a_starting_point) # if needed, e.g. we are solving a nonlinear problem by optimization`\n\n    \u00a0\n\n-   `InversionOptions` describes details about how one wants to run the\n    inversion, including the backend tool and solver-specific\n    parameters. It is based on the concept of a `method` and `tool`.\n\n    ``` python\n    inv_options = InversionOptions()\n    inv_options.suggest_solving_methods()\n    inv_options.set_solving_method(\"matrix solvers\")\n    inv_options.suggest_tools()\n    inv_options.set_tool(\"scipy.linalg.lstsq\")\n    inv_options.summary()\n    ```\n\n    \u00a0\n\n-   `Inversion` can be seen as an inversion engine that takes in the\n    above two as information, and will produce an `InversionResult` upon\n    running.\n\n    ``` python\n    inv = Inversion(inv_problem, inv_options)\n    result = inv.run()\n    ```\n\nInternally CoFI decides the nature of the problem from the quantities\nset by the user and performs internal checks to ensure it has all that\nit needs to solve a problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Linear system solver\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cofi import BaseProblem, InversionOptions, Inversion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 1. Define CoFI `BaseProblem`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inv_problem = BaseProblem()\ninv_problem.set_data(data_y)\ninv_problem.set_jacobian(jacobian())\ninv_problem.set_data_covariance_inv(Cd_inv())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 2. Define CoFI `InversionOptions`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inv_options = InversionOptions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the information supplied, we can ask CoFI to suggest some solving\nmethods.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inv_options.suggest_solving_methods()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can ask CoFI to suggest some specific software tools as well.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inv_options.suggest_tools()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inv_options.set_solving_method(\"matrix solvers\") # lets decide to use a matrix solver.\ninv_options.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# below is optional, as this has already been the default tool under \"linear least square\"\ninv_options.set_tool(\"scipy.linalg.lstsq\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 3. Define CoFI `Inversion` and run\n\nOur choices so far have defined a linear parameter estimation problem\n(without any regularization) to be solved within a least squares\nframework. In this case the selection of a `matrix solvers` method will\nmean we are calculating the standard least squares solution\n\n$$m = (G^T C_d^{-1} G)^{-1} G^T C_d^{-1} d$$\n\nand our choice of backend tool `scipy.linalg.lstsq`, means that we will\nemploy scipy's `linalg` package to perform the numerics.\n\nLets run CoFI.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inv = Inversion(inv_problem, inv_options)\ninv_result = inv.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"The inversion result from `scipy.linalg.lstsq`: {inv_result.model}\\n\")\ninv_result.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets plot the solution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_data(title=\"Eustatic sea-level\")\nplot_model(data_x,jacobian(data_x).dot(inv_result.model), \"linear system solver\", color=\"seagreen\")\nplot_model(ref_x,ref_y, \"Reference model\", color=\"darkorange\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Optimizer\n\nThe same overdetermined linear problem, $\\textbf{d} = G\\textbf{m}$, with\nGaussian data noise can also be solved by minimising the squares of the\nresidual of the linear equations,\ne.g.\u00a0$\\textbf{r}^T \\textbf{C}_d^{-1}\\textbf{r}$ where\n$\\textbf{r}=\\textbf{d}-G\\textbf{m}$. The above matrix solver solution\ngives us the best data fitting model, but a direct optimisation approach\ncould also be used, say when the number of unknowns is large and we do\nnot wish, or are unable to provide the Jacobian function.\n\nSo we use a plain optimizer `scipy.optimize.minimize` to demonstrate\nthis ability.\n\n```{=html}\n<!-- For this backend solver to run successfully, some additional information should be provided, otherwise\nyou'll see an error to notify what additional information is required by the solver.\n\nThere are several ways to provide the information needed to solve an inverse problem with \nCoFI. In the example below we provide functions to calculate the data and the optional \nregularisation. CoFI then generates the objective function for us based on the information \nprovided. The alternative to this would be to directly provide objective function to CoFI. -->\n```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "######## CoFI BaseProblem - provide additional information\ninv_problem.set_initial_model(np.ones(nparams))\n#inv_problem.set_initial_model(inv_result.model)\ninv_problem.set_forward(forward)\ninv_problem.set_data_misfit(\"squared error\")\n\n# inv_problem.set_objective(your_own_misfit_function)    # (optionally) if you'd like to define your own misfit\n# inv_problem.set_gradient(your_own_gradient_of_misfit_function)    # (optionally) if you'd like to define your own misfit gradient\n\n######## CoFI InversionOptions - set a different tool\ninv_options_2 = InversionOptions()\ninv_options_2.set_tool(\"scipy.optimize.minimize\")\ninv_options_2.set_params(method=\"Nelder-Mead\")\n\n######## CoFI Inversion - run it\ninv_2 = Inversion(inv_problem, inv_options_2)\ninv_result_2 = inv_2.run()\n\n######## CoFI InversionResult - check result\nprint(f\"The inversion result from `scipy.optimize.minimize`: {inv_result_2.model}\\n\")\ninv_result_2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_data()\nplot_model(data_x,jacobian(data_x).dot(inv_result_2.model), \"optimization solution\", color=\"cornflowerblue\")\nplot_model(ref_x,ref_y, \"Reference model\", color=\"darkorange\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The optimization fails to convergence for this problem (with default\nsettings).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Challenge - Change the polynomial degree\n\nTry and replace the 3rd order polynomial with a 2nd order polynomial\n(i.e.\u00a0$M=2$) by adding the required commands below. What does the plot\nlooks like?\n\nStart from code below:\n\n    inv_problem = BaseProblem()\n    inv_problem.set_data(data_y)\n    inv_problem.set_jacobian(jacobian(n=<CHANGE ME>))\n    inv_problem.set_data_covariance_inv(Cd_inv())\n    inv_options.set_solving_method(\"matrix solvers\") # lets decide to use a matrix solver.\n    inv = Inversion(inv_problem, inv_options)\n    inv_result = inv.run()\n\n    print(\"Inferred curve with n = <CHANGE ME> \")\n    plot_data()\n    plot_model(data_x,jacobian(x,n=<CHANGE ME>).dot(inv_result.model), \"optimization solution\", color=\"cornflowerblue\")\n    plot_model(ref_x,ref_y, \"Reference model\", color=\"darkorange\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Copy the template above, Replace <CHANGE ME> with your answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#@title Solution\n\ninv_problem = BaseProblem()\ninv_problem.set_data(data_y)\ninv_problem.set_jacobian(jacobian(n=3))\ninv_problem.set_data_covariance_inv(Cd_inv())\ninv_options.set_solving_method(\"matrix solvers\") # lets decide to use a matrix solver.\ninv = Inversion(inv_problem, inv_options)\ninv_result = inv.run()\n\nprint(\"Inferred curve with n = 3 \")\nplot_data()\nplot_model(data_x,jacobian(data_x,n=3).dot(inv_result.model), \"optimization solution\", color=\"cornflowerblue\")\nplot_model(ref_x,ref_y, \"Reference model\", color=\"darkorange\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Changing to a second order polynomial does converge but gives a poor\nfit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Bayesian sampling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Likelihood\n\nSince data errors follow a Gaussian in this example, we can define a\nLikelihood function, $p({\\mathbf d}_{obs}| {\\mathbf m})$.\n\n$$p({\\mathbf d}_{obs} | {\\mathbf m}) \\propto \\exp \\left\\{- \\frac{1}{2} ({\\mathbf d}_{obs}-{\\mathbf d}_{pred}({\\mathbf m}))^T C_D^{-1} ({\\mathbf d}_{obs}-{\\mathbf d}_{pred}({\\mathbf m})) \\right\\}$$\n\nwhere ${\\mathbf d}_{obs}$ represents the observed y values and\n${\\mathbf d}_{pred}({\\mathbf m})$ are those predicted by the polynomial\nmodel $({\\mathbf m})$. The Likelihood is defined as the probability of\nobserving the data actually observed, given a model. In practice we\nusually only need to evaluate the log of the Likelihood,\n$\\log p({\\mathbf d}_{obs} | {\\mathbf m})$. To do so, we require the\ninverse data covariance matrix describing the statistics of the noise in\nthe data, $C_D^{-1}$ . For this problem the data errors are independent\nwith identical standard deviation in noise for each datum. Hence\n$C_D^{-1} = \\frac{1}{\\sigma^2}I$ where $\\sigma=1$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we artificially increase the observational errors on the data so\nthat the spread of the posterior samples are visible.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Cdinv = Cd_inv()                 # inverse data covariance matrix\n\ndef log_likelihood(model):\n    y_synthetics = forward(model)\n    residual = data_y - y_synthetics\n    return -0.5 * residual @ (Cdinv @ residual).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the user could specify **any appropriate Likelihood function**\nof their choosing here.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prior\n\nBayesian sampling requires a prior probability density function. A\ncommon problem with polynomial coefficients as model parameters is that\nit is not at all obvious what a prior should be. Here we choose a\nuniform prior with specified bounds\n\n$$\\begin{aligned}\n\\begin{align}\np({\\mathbf m}) &= \\frac{1}{V},\\quad  l_i \\le m_i \\le u_i, \\quad (i=1,\\dots,M)\\\\\n\\\\\n         &= 0, \\quad {\\rm otherwise},\n\\end{align}\n\\end{aligned}$$\n\nwhere $l_i$ and $u_i$ are lower and upper bounds on the $i$th model\ncoefficient.\n\nHere use the uniform distribution with\n${\\mathbf l}^T = (-10.,-10.,-10.,-10.)$, and\n${\\mathbf u}^T = (10.,10.,10.,10.)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "m_lower_bound = np.ones(nparams) * (-10.)             # lower bound for uniform prior\nm_upper_bound = np.ones(nparams) * 10                 # upper bound for uniform prior\n\ndef log_prior(model):    # uniform distribution\n    for i in range(len(m_lower_bound)):\n        if model[i] < m_lower_bound[i] or model[i] > m_upper_bound[i]: return -np.inf\n    return 0.0 # model lies within bounds -> return log(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the user could specify **any appropriate Prior PDF** of their\nchoosing here.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bayesian sampling\n\nIn this aproach we sample a probability distribution rather than find a\nsingle best fit solution. Bayes' theorem tells us the the posterior\ndistribution is proportional to the Likelihood and the prior.\n\n$$p(\\mathbf{m}|\\mathbf{d}) = K p(\\mathbf{d}|\\mathbf{m})p(\\mathbf{m})$$\n\nwhere $K$ is some constant. Under the assumptions specified\n$p(\\mathbf{m}|\\mathbf{d})$ gives a probability density of models that\nare supported by the data. We seek to draw random samples from\n$p(\\mathbf{m}|\\mathbf{d})$ over model space and then to make inferences\nfrom the resulting ensemble of model parameters.\n\nIn this example we make use of *The Affine Invariant Markov chain Monte\nCarlo (MCMC) Ensemble sampler* [Goodman and Weare\n2010](https://msp.org/camcos/2010/5-1/p04.xhtml) to sample the posterior\ndistribution of the model. (See more details about\n[emcee](https://emcee.readthedocs.io/en/stable/)).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Starting points for random walkers\n\nNow we define some hyperparameters (e.g.\u00a0the number of walkers and\nsteps), and initialise the starting positions of walkers. We start all\nwalkers in a small ball about a chosen point $(0, 0, 0, 0)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nwalkers = 32\nndim = nparams\nnsteps = 10000\nwalkers_start = np.zeros(nparams) + 1e-4 * np.random.randn(nwalkers, ndim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Add the information and run with CoFI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "######## CoFI BaseProblem - provide additional information\ninv_problem.set_log_prior(log_prior)\ninv_problem.set_log_likelihood(log_likelihood)\ninv_problem.set_model_shape(ndim)\n\n######## CoFI InversionOptions - get a different tool\ninv_options_3 = InversionOptions()\ninv_options_3.set_tool(\"emcee\")      # Here we use to Affine Invariant McMC sampler from Goodman and Weare (2010).\ninv_options_3.set_params(nwalkers=nwalkers, nsteps=nsteps, progress=True, initial_state=walkers_start)\n\n######## CoFI Inversion - run it\ninv_3 = Inversion(inv_problem, inv_options_3)\ninv_result_3 = inv_3.run()\n\n######## CoFI InversionResult - check result\nprint(f\"The inversion result from `emcee`:\")\ninv_result_3.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Post-sampling analysis\n\nBy default the raw sampler resulting object is attached to `cofi`'s\ninversion result.\n\nOptionally, you can convert that into an `arviz` data structure to have\naccess to a range of analysis functions. (See more details in [arviz\ndocumentation](https://python.arviz.org/en/latest/index.html)).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import arviz as az\n\nlabels = [\"m0\", \"m1\", \"m2\",\"m3\",\"m4\"]\n\nsampler = inv_result_3.sampler\naz_idata = az.from_emcee(sampler, var_names=labels)\n# az_idata = inv_result_3.to_arviz()      # alternatively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "az_idata.get(\"posterior\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# a standard `trace` plot\naxes = az.plot_trace(az_idata, backend_kwargs={\"constrained_layout\":True}); \n\n# add legends\nfor i, axes_pair in enumerate(axes):\n    ax1 = axes_pair[0]\n    ax2 = axes_pair[1]\n    #ax1.axvline(true_model[i], linestyle='dotted', color='red')\n    ax1.set_xlabel(\"parameter value\")\n    ax1.set_ylabel(\"density value\")\n    ax2.set_xlabel(\"number of iterations\")\n    ax2.set_ylabel(\"parameter value\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#tau = sampler.get_autocorr_time()\n#print(f\"autocorrelation time: {tau}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# a Corner plot\n\nfig, axes = plt.subplots(nparams, nparams, figsize=(12,8))\n\nif(True): # if we are plotting the model ensemble use this\n    az.plot_pair(\n        az_idata.sel(draw=slice(300,None)), \n        marginals=True, \n        #reference_values=dict(zip([f\"m{i}\" for i in range(4)], true_model.tolist())),\n        ax=axes,\n    );\nelse: # if we wish to plot a kernel density plot then use this option\n    az.plot_pair(\n        az_idata.sel(draw=slice(300,None)), \n        marginals=True, \n        #reference_values=dict(zip([f\"m{i}\" for i in range(4)], true_model.tolist())),\n        kind=\"kde\",\n        kde_kwargs={\n            \"hdi_probs\": [0.3, 0.6, 0.9],  # Plot 30%, 60% and 90% HDI contours\n            \"contourf_kwargs\": {\"cmap\": \"Blues\"},\n        },\n        ax=axes,\n    );"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we plot the predicted curves for the posterior ensemble of\nsolutions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "flat_samples = sampler.get_chain(discard=300, thin=30, flat=True)\ninds = np.random.randint(len(flat_samples), size=100) # get a random selection from posterior ensemble\n\nplot_data(title=\"Eustatic sea-level\")\nplt.xlim(0,maxtime)\nplot_models(flat_samples[inds],color=\"lightgrey\")\nplot_model(ref_x,ref_y, \"Reference model\", color=\"darkorange\")\n#plt.xlim(15,20.)\n#plt.ylim(-140,-100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Expected values, credible intervals and model covariance matrix from the ensemble\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n Expected value and 95% credible intervals \")\nfor i in range(ndim):\n    mcmc = np.percentile(flat_samples[:, i], [5, 50, 95])\n    print(\" {} {:7.3f} [{:7.3f}, {:7.3f}]\".format(labels[i],mcmc[1],mcmc[0],mcmc[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "CMpost = np.cov(flat_samples.T)\nCM_std= np.std(flat_samples,axis=0)\nprint('Posterior model covariance matrix\\n',CMpost)\nprint('\\n Posterior estimate of model standard deviations in each parameter')\nfor i in range(ndim):\n    print(\"    {} {:7.4f}\".format(labels[i],CM_std[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Challenge - Change the prior model bounds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Replace the previous prior bounds to new values\n\nThe original uniform bounds had\n\n${\\mathbf l}^T = (-10.,-10.,-10.,-10.)$, and\n${\\mathbf u}^T = (10.,10.,10.,10.)$.\n\nLets replace with\n\n${\\mathbf l}^T = (-0.5,-10.,-10.,-10.)$, and\n${\\mathbf u}^T = (0.5,10.,10.,10.)$.\n\nWe have only changed the bounds of the first parameter. However since\nthe true value of constant term was 6, these bounds are now inconsistent\nwith the true model.\n\nWhat does this do to the posterior distribution?\n\nStart from the code template below:\n\n    m_lower_bound = <CHANGE ME>             # lower bound for uniform prior\n    m_upper_bound = <CHANGE ME>             # upper bound for uniform prior\n\n    def log_prior(model):    # uniform distribution\n        for i in range(len(m_lower_bound)):\n            if model[i] < m_lower_bound[i] or model[i] > m_upper_bound[i]: return -np.inf\n        return 0.0 # model lies within bounds -> return log(1)\n\n    ######## CoFI BaseProblem - update information\n    inv_problem.set_log_prior(log_prior)\n\n    ######## CoFI Inversion - run it\n    inv_4 = Inversion(inv_problem, inv_options_3)\n    inv_result_4 = inv_4.run()\n\n    flat_samples = inv_result_4.sampler.get_chain(discard=300, thin=30, flat=True)\n    inds = np.random.randint(len(flat_samples), size=100) # get a random selection from posterior ensemble\n\n    print(\"Resulting samples with prior model lower bounds of <CHANGE ME>, upper bounds of <CHANGE ME>\")\n    plot_data()\n    plot_models(flat_samples[inds])\n    plot_model(x, true_y, \"True model\", color=\"darkorange\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Copy the template above, Replace <CHANGE ME> with your answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#@title Solution\n\nm_lower_bound = np.array([-1.0,-10,-10,-10])             # lower bound for uniform prior\nm_upper_bound = np.array([1.0,10,10,10])                 # upper bound for uniform prior\n\ndef log_prior(model):    # uniform distribution\n    for i in range(len(m_lower_bound)):\n        if model[i] < m_lower_bound[i] or model[i] > m_upper_bound[i]: return -np.inf\n    return 0.0 # model lies within bounds -> return log(1)\n\n######## CoFI BaseProblem - update information\ninv_problem.set_log_prior(log_prior)\n\n######## CoFI Inversion - run it\ninv_4 = Inversion(inv_problem, inv_options_3)\ninv_result_4 = inv_4.run()\n\nflat_samples = inv_result_4.sampler.get_chain(discard=300, thin=30, flat=True)\ninds = np.random.randint(len(flat_samples), size=100) # get a random selection from posterior ensemble\n\nprint(\"Resulting samples with prior model lower bounds of [-1,-10,-10,-10], upper bounds of [2,10,10,10]\")\nplot_data()\nplot_models(flat_samples[inds],color=\"lightgrey\")\nplot_model(ref_x, ref_y, \"Reference model\", color=\"darkorange\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Is there much change to the posterior distribution?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n\n# Watermark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "watermark_list = [\"cofi\", \"numpy\", \"scipy\", \"matplotlib\", \"emcee\", \"arviz\"]\nfor pkg in watermark_list:\n    pkg_var = __import__(pkg)\n    print(pkg, getattr(pkg_var, \"__version__\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "sphinx_gallery_thumbnail_number = -1\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}